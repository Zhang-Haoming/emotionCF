{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Emotion CF A Python package for collaborative filtering on emotion datasets Installation pip install git+https://github.com/cosanlab/emotionCF.git Getting started Checkout the quick overview for examples to help you get started. Or check out the API reference on the left to explore the details of specific models. A unique feature of this toolbox is its support for working with time-series data . Algorithms Currently supported algorithms include: Mean - a baseline model KNN - k-nearest neighbors NNMF_mult - non-negative matrix factorization trained via multiplicative updating NNMF_sgd - non-negative matrix factorization trained via stochastic gradient descent","title":"Home"},{"location":"#emotion-cf","text":"","title":"Emotion CF"},{"location":"#a-python-package-for-collaborative-filtering-on-emotion-datasets","text":"","title":"A Python package for collaborative filtering on emotion datasets"},{"location":"#installation","text":"pip install git+https://github.com/cosanlab/emotionCF.git","title":"Installation"},{"location":"#getting-started","text":"Checkout the quick overview for examples to help you get started. Or check out the API reference on the left to explore the details of specific models. A unique feature of this toolbox is its support for working with time-series data .","title":"Getting started"},{"location":"#algorithms","text":"Currently supported algorithms include: Mean - a baseline model KNN - k-nearest neighbors NNMF_mult - non-negative matrix factorization trained via multiplicative updating NNMF_sgd - non-negative matrix factorization trained via stochastic gradient descent","title":"Algorithms"},{"location":"development/","text":"Development Each new push or pull-request to the code base for this toolbox will automatically be run through testing and documentation building via github actions. To develop this package or its documentation locally you will need to install a few extra dependencies. Installation pip install -r requirements-dev.txt Testing To run tests just call pytest from the root of this repository. New tests can be added in emotioncf/tests/ . Formatting Please format your code using black. If you've installed the development dependencies, then you can configure git to tell if you any new changes are not formatted by setting up a pre-commit hook: cd .git/hooks Create a new file called pre-commit with the following contents: #!/bin/sh black --check . - Make sure the file is executable chmod 775 pre-commit Now anytime you try to commit new changes, git will automatically run black before the commit and warn you if certain files need to be formatted. Editing continuous integration To change how the automatic workflow builds are specified, make the relevant edits in .github/workflows/conda_ci.yml . Documentation Documentation is built with mkdocs using the mkdocs material theme and mkdocstrings extension. Live server After installation above, simply run mkdocs serve this the project root to start a hot-reloading server of the documentation at http://localhost:8000 . To alter the layout of the docs site adjust settings in mkdocs.yml . To add or edit pages simply create markdown files within the docs/ folder. Deploying You can use the mkdocs gh-deploy command in order to build and push the documentation site to the github-pages branch of this repo.","title":"Development"},{"location":"development/#development","text":"Each new push or pull-request to the code base for this toolbox will automatically be run through testing and documentation building via github actions. To develop this package or its documentation locally you will need to install a few extra dependencies.","title":"Development"},{"location":"development/#installation","text":"pip install -r requirements-dev.txt","title":"Installation"},{"location":"development/#testing","text":"To run tests just call pytest from the root of this repository. New tests can be added in emotioncf/tests/ .","title":"Testing"},{"location":"development/#formatting","text":"Please format your code using black. If you've installed the development dependencies, then you can configure git to tell if you any new changes are not formatted by setting up a pre-commit hook: cd .git/hooks Create a new file called pre-commit with the following contents: #!/bin/sh black --check . - Make sure the file is executable chmod 775 pre-commit Now anytime you try to commit new changes, git will automatically run black before the commit and warn you if certain files need to be formatted.","title":"Formatting"},{"location":"development/#editing-continuous-integration","text":"To change how the automatic workflow builds are specified, make the relevant edits in .github/workflows/conda_ci.yml .","title":"Editing continuous integration"},{"location":"development/#documentation","text":"Documentation is built with mkdocs using the mkdocs material theme and mkdocstrings extension.","title":"Documentation"},{"location":"development/#live-server","text":"After installation above, simply run mkdocs serve this the project root to start a hot-reloading server of the documentation at http://localhost:8000 . To alter the layout of the docs site adjust settings in mkdocs.yml . To add or edit pages simply create markdown files within the docs/ folder.","title":"Live server"},{"location":"development/#deploying","text":"You can use the mkdocs gh-deploy command in order to build and push the documentation site to the github-pages branch of this repo.","title":"Deploying"},{"location":"overview/","text":"Quick Overview Example Usage All algorithms in the emotionCF toolbox operates on 2d pandas dataframes with rows as unique users and columns as unique items . Create a subject by item matrix The toolbox contains a helper function create_sub_by_item_matrix() to convert a long-form dataframe into this format provided it has 3 columns named ['Subject', 'Item', 'Rating'] or equivalent (see the columns keyword argument of create_sub_by_item_matrix ). from emotioncf import create_sub_by_item_matrix ratings = create_sub_by_item_matrix ( long_format_df ) Initialize a model instance Before you can fit a model on your data you need to initialize a new model instance by passing in your user x item dataframe: from emotioncf import KNN cf = KNN ( ratings ) Split Data into Train and Test It is easy to split your data into training and test sets using the split_train_test() method on any model instance. This creates a binary mask saved into the model object itself and accessible at model.train_mask . Unlike libraries like sklearn , splitting data this way chooses a different random subset of items per user to perform training. All users are always included in training set, but each user will have a different random set of items . # use 50% of the items for training per user cf . split_train_test ( n_train_items =. 5 ) # contains the training mask: cf . train_mask Estimate Model Each model can be estimated using its fit() method. If no data splitting is done, then all data is used for training. cf . fit () Predict New Ratings To generate predictions, used a model's predict() method. This creates a pandas dataframe of the predicted subject by item matrix within the model object itself, accessible at model.predicted_ratings . cf . predict () Evaluate Model Predictions There are several methods to aid in evaluating the performance of the model including: - overall mean squared error .get_mse() - overall correlation .get_corr() - mean-squared-error separately per user .get_sub_mse() - correlation for each subject .get_sub_corr() Additionally, each method takes a string argument to indicate what dataset performance should be calculated for: 'train' , 'test' , or 'all' . Defaults to 'test' . cf . get_mse ( 'all' ) cf . get_corr () # 'test' cf . get_sub_corr ( 'train' ) Working with Time-Series Data A unique feature of this toolbox, is that it has also been designed to work with time-series data. For example, model instances can be downsampled across items, where items refers to time samples. You must specify the sampling_freq of the data, and the target , where target must have a target_type of ['hz','samples','seconds'] . Downsampling is performed by averaging over bin windows. In this example we downsample a dataset from 10Hz to 5Hz. cf . downsample ( sampling_freq = 10 , target = 5 , target_type = 'hz' ) It is also possible to leverage presumed autocorrelation when training models by using the dilate_ts_n_samples=n_samples keyword. This flag will convolve a boxcar kernel of width n_samples with each user's rating from model.train_mask . The dilation will be centered on each sample. The intuition here is that if a subject rates an item at a given time point, say '50', they likely will have rated time points immediately preceding and following similarly (e.g., [50,50,50] ). This is due to autocorrelation in the data. More presumed autocorrelation will likely benefit from a higher number of samples being selected. This will allow time series that are sparsely sampled to be estimated more accurately. cf = NNMF_sgd ( ratings ) mask = cf . train_mask cf . split_train_test ( n_train_items = 20 ) cf . fit ( n_iterations = 100 , user_fact_reg = 1.0 , item_fact_reg = 0.001 , user_bias_reg = 0 , item_bias_reg = 0 , learning_rate =. 001 , dilate_ts_n_samples = 20 ) cf . predict () cf . get_mse ( 'test' ) cf . get_corr ( 'test' ) cf . plot_predictions () Supported Algorithms Mean An easy control model for collaborative filtering is to demonstrate how well the models perform over simply using the item means. We initialize a class instance and then the model can be estimated and new ratings predicted. We can get the overall mean squared error on the predicted ratings. from emotioncf.cf import Mean cf = Mean ( ratings ) cf . fit () cf . predict () cf . get_mse ( 'all' ) K-Nearest Neighbors EmotionCF uses a standard API to estimate and predict data. Though the KNN approach is not technically a model, we still use the fit method to estimate data. This calculates a similarity matrix between subjects using ['correlation','cosine'] methods. We can then predict the left out ratings using the top k nearest neighbors. We can evaluate how well the model works for all data points using get_corr() and get_mse() methods. We can also get the correlation for each subject's individual data using get_sub_corr() method. So far we have found that this method does not perform well when there aren't many overlapping samples across items and users. from emotioncf import KNN cf = KNN ( ratings ) cf . split_train_test ( n_train_items = 20 ) cf . fit ( metric = 'pearson' ) cf . predict ( k = 10 ) cf . get_mse ( 'test' ) cf . get_corr ( 'test' ) cf . get_sub_corr ( 'test' ) Non-negative matrix factorization using stochastic gradient descent Here we initialize a new class instance and split the data into 20 training and 80 test items per subject. We fit the model using 100 iterations. Can pass in optional regularization parameters and a learning rate for the update function. The model is then used to predict the left out ratings. We can get the overall model MSE and correlation value on the test ratings. We can also make a quick plot of the results. As indicated by the name, this method does not work with data that includes negative numbers. from emotioncf import NNMF_sgd cf = NNMF_sgd ( ratings ) cf . split_train_test ( n_train_items = 20 ) cf . fit ( n_iterations = 100 , user_fact_reg = 0 , item_fact_reg = 0 , user_bias_reg = 0 , item_bias_reg = 0 , learning_rate =. 001 ) cf . predict () cf . get_mse ( 'test' ) cf . get_corr ( 'test' ) cf . plot_predictions () Non-negative matrix factorization using multiplicative updating Similarly, we can fit a different NNMF model that uses multiplicative updating with the NNMF_mult class. from emotioncf import NNMF_mult cf = NNMF_mult ( ratings ) cf . split_train_test ( n_train_items = 20 ) cf . fit ( max_iterations = 200 ) cf . predict () cf . get_mse ( 'test' ) cf . get_corr ( 'test' ) cf . plot_predictions ()","title":"Quick Overview"},{"location":"overview/#quick-overview","text":"","title":"Quick Overview"},{"location":"overview/#example-usage","text":"All algorithms in the emotionCF toolbox operates on 2d pandas dataframes with rows as unique users and columns as unique items .","title":"Example Usage"},{"location":"overview/#create-a-subject-by-item-matrix","text":"The toolbox contains a helper function create_sub_by_item_matrix() to convert a long-form dataframe into this format provided it has 3 columns named ['Subject', 'Item', 'Rating'] or equivalent (see the columns keyword argument of create_sub_by_item_matrix ). from emotioncf import create_sub_by_item_matrix ratings = create_sub_by_item_matrix ( long_format_df )","title":"Create a subject by item matrix"},{"location":"overview/#initialize-a-model-instance","text":"Before you can fit a model on your data you need to initialize a new model instance by passing in your user x item dataframe: from emotioncf import KNN cf = KNN ( ratings )","title":"Initialize a model instance"},{"location":"overview/#split-data-into-train-and-test","text":"It is easy to split your data into training and test sets using the split_train_test() method on any model instance. This creates a binary mask saved into the model object itself and accessible at model.train_mask . Unlike libraries like sklearn , splitting data this way chooses a different random subset of items per user to perform training. All users are always included in training set, but each user will have a different random set of items . # use 50% of the items for training per user cf . split_train_test ( n_train_items =. 5 ) # contains the training mask: cf . train_mask","title":"Split Data into Train and Test"},{"location":"overview/#estimate-model","text":"Each model can be estimated using its fit() method. If no data splitting is done, then all data is used for training. cf . fit ()","title":"Estimate Model"},{"location":"overview/#predict-new-ratings","text":"To generate predictions, used a model's predict() method. This creates a pandas dataframe of the predicted subject by item matrix within the model object itself, accessible at model.predicted_ratings . cf . predict ()","title":"Predict New Ratings"},{"location":"overview/#evaluate-model-predictions","text":"There are several methods to aid in evaluating the performance of the model including: - overall mean squared error .get_mse() - overall correlation .get_corr() - mean-squared-error separately per user .get_sub_mse() - correlation for each subject .get_sub_corr() Additionally, each method takes a string argument to indicate what dataset performance should be calculated for: 'train' , 'test' , or 'all' . Defaults to 'test' . cf . get_mse ( 'all' ) cf . get_corr () # 'test' cf . get_sub_corr ( 'train' )","title":"Evaluate Model Predictions"},{"location":"overview/#working-with-time-series-data","text":"A unique feature of this toolbox, is that it has also been designed to work with time-series data. For example, model instances can be downsampled across items, where items refers to time samples. You must specify the sampling_freq of the data, and the target , where target must have a target_type of ['hz','samples','seconds'] . Downsampling is performed by averaging over bin windows. In this example we downsample a dataset from 10Hz to 5Hz. cf . downsample ( sampling_freq = 10 , target = 5 , target_type = 'hz' ) It is also possible to leverage presumed autocorrelation when training models by using the dilate_ts_n_samples=n_samples keyword. This flag will convolve a boxcar kernel of width n_samples with each user's rating from model.train_mask . The dilation will be centered on each sample. The intuition here is that if a subject rates an item at a given time point, say '50', they likely will have rated time points immediately preceding and following similarly (e.g., [50,50,50] ). This is due to autocorrelation in the data. More presumed autocorrelation will likely benefit from a higher number of samples being selected. This will allow time series that are sparsely sampled to be estimated more accurately. cf = NNMF_sgd ( ratings ) mask = cf . train_mask cf . split_train_test ( n_train_items = 20 ) cf . fit ( n_iterations = 100 , user_fact_reg = 1.0 , item_fact_reg = 0.001 , user_bias_reg = 0 , item_bias_reg = 0 , learning_rate =. 001 , dilate_ts_n_samples = 20 ) cf . predict () cf . get_mse ( 'test' ) cf . get_corr ( 'test' ) cf . plot_predictions ()","title":"Working with Time-Series Data"},{"location":"overview/#supported-algorithms","text":"","title":"Supported Algorithms"},{"location":"overview/#mean","text":"An easy control model for collaborative filtering is to demonstrate how well the models perform over simply using the item means. We initialize a class instance and then the model can be estimated and new ratings predicted. We can get the overall mean squared error on the predicted ratings. from emotioncf.cf import Mean cf = Mean ( ratings ) cf . fit () cf . predict () cf . get_mse ( 'all' )","title":"Mean"},{"location":"overview/#k-nearest-neighbors","text":"EmotionCF uses a standard API to estimate and predict data. Though the KNN approach is not technically a model, we still use the fit method to estimate data. This calculates a similarity matrix between subjects using ['correlation','cosine'] methods. We can then predict the left out ratings using the top k nearest neighbors. We can evaluate how well the model works for all data points using get_corr() and get_mse() methods. We can also get the correlation for each subject's individual data using get_sub_corr() method. So far we have found that this method does not perform well when there aren't many overlapping samples across items and users. from emotioncf import KNN cf = KNN ( ratings ) cf . split_train_test ( n_train_items = 20 ) cf . fit ( metric = 'pearson' ) cf . predict ( k = 10 ) cf . get_mse ( 'test' ) cf . get_corr ( 'test' ) cf . get_sub_corr ( 'test' )","title":"K-Nearest Neighbors"},{"location":"overview/#non-negative-matrix-factorization-using-stochastic-gradient-descent","text":"Here we initialize a new class instance and split the data into 20 training and 80 test items per subject. We fit the model using 100 iterations. Can pass in optional regularization parameters and a learning rate for the update function. The model is then used to predict the left out ratings. We can get the overall model MSE and correlation value on the test ratings. We can also make a quick plot of the results. As indicated by the name, this method does not work with data that includes negative numbers. from emotioncf import NNMF_sgd cf = NNMF_sgd ( ratings ) cf . split_train_test ( n_train_items = 20 ) cf . fit ( n_iterations = 100 , user_fact_reg = 0 , item_fact_reg = 0 , user_bias_reg = 0 , item_bias_reg = 0 , learning_rate =. 001 ) cf . predict () cf . get_mse ( 'test' ) cf . get_corr ( 'test' ) cf . plot_predictions ()","title":"Non-negative matrix factorization using stochastic gradient descent"},{"location":"overview/#non-negative-matrix-factorization-using-multiplicative-updating","text":"Similarly, we can fit a different NNMF model that uses multiplicative updating with the NNMF_mult class. from emotioncf import NNMF_mult cf = NNMF_mult ( ratings ) cf . split_train_test ( n_train_items = 20 ) cf . fit ( max_iterations = 200 ) cf . predict () cf . get_mse ( 'test' ) cf . get_corr ( 'test' ) cf . plot_predictions ()","title":"Non-negative matrix factorization using multiplicative updating"},{"location":"api/base/","text":"emotioncf.base Base algorithm classes. All other algorithms inherit from these classes which means they have access to all their methods and attributes. You won't typically utilize these classes directly unless you're creating a custom estimator. Base All other models and base classes inherit from this class. create_masked_data ( self , n_mask_items = 0.1 ) Create a mask and apply it to data using number of items or % of items Parameters: Name Type Description Default n_itmes int/float if an integer is passed its raw value is used. Otherwise if a float is passed its taken to be a (rounded) percentage of the total items; Default .1 (10% of the data) required Source code in emotioncf/base.py def create_masked_data ( self , n_mask_items = 0.1 ): \"\"\" Create a mask and apply it to data using number of items or % of items Args: n_itmes (int/float, optional): if an integer is passed its raw value is used. Otherwise if a float is passed its taken to be a (rounded) percentage of the total items; Default .1 (10% of the data) \"\"\" if ( isinstance ( n_mask_items , np . floating ) and ( n_mask_items >= 1.0 or n_mask_items <= 0.0 ) ) or ( isinstance ( n_mask_items , int ) and ( n_mask_items >= self . data . shape [ 1 ] or n_mask_items <= 0 ) ): raise TypeError ( \"n_items should a float between 0-1 or an integer < the number of items\" ) self . mask = create_train_test_mask ( self . data , n_mask_items ) self . masked_data = self . data [ self . mask ] self . is_masked = True self . n_mask_items = n_mask_items dilate_mask ( self , n_samples = None ) Dilate sparse time-series data by n_samples. Overlapping data will be averaged. This method computes and stores the dilated mask in .dilated_mask and internally updates the .masked_data . Repeated calls to this method on the same model instance do not stack, but rather perform a new dilation on the original masked data. Called this method with None will undo any dilation. This is an alias to ._dilate_ts_rating_samples Parameters: Name Type Description Default nsamples int Number of samples to dilate data required Source code in emotioncf/base.py def dilate_mask ( self , n_samples = None ): \"\"\"Dilate sparse time-series data by n_samples. Overlapping data will be averaged. This method computes and stores the dilated mask in `.dilated_mask` and internally updates the `.masked_data`. Repeated calls to this method on the same model instance **do not** stack, but rather perform a new dilation on the original masked data. Called this method with `None` will undo any dilation. This is an alias to `._dilate_ts_rating_samples` Args: nsamples (int): Number of samples to dilate data \"\"\" self . _dilate_ts_rating_samples ( n_samples = n_samples ) downsample ( self , n_samples , sampling_freq = None , target_type = 'samples' ) Downsample a model's rating matrix to a new target frequency or number of samples using averaging. Also downsamples a model's mask and dilated mask if they exist as well as a model's predictions if it's already been fit. If target_type = 'samples' and sampling_freq is None, the new user x item matrix will have shape users x items * (1 / n_samples). If target_type = 'seconds', the new user x item matrix will have shape users x items * (1 / n_samples * sampling_freq). If target_type = 'hz', the new user x item matrix will have shape users x items * (1 / sampling_freq / n_samples). Parameters: Name Type Description Default n_samples int number of samples required sampling_freq int/float Sampling frequency of data; Default None None target_type str how to downsample; must be one of \"samples\", \"seconds\" or \"hz\". Defaults to \"samples\". 'samples' Source code in emotioncf/base.py def downsample ( self , n_samples , sampling_freq = None , target_type = \"samples\" ): \"\"\" Downsample a model's rating matrix to a new target frequency or number of samples using averaging. Also downsamples a model's mask and dilated mask if they exist as well as a model's predictions if it's already been fit. If target_type = 'samples' and sampling_freq is None, the new user x item matrix will have shape users x items * (1 / n_samples). If target_type = 'seconds', the new user x item matrix will have shape users x items * (1 / n_samples * sampling_freq). If target_type = 'hz', the new user x item matrix will have shape users x items * (1 / sampling_freq / n_samples). Args: n_samples (int): number of samples sampling_freq (int/float): Sampling frequency of data; Default None target_type (str, optional): how to downsample; must be one of \"samples\", \"seconds\" or \"hz\". Defaults to \"samples\". \"\"\" self . data = downsample_dataframe ( self . data , sampling_freq = sampling_freq , n_samples = n_samples , target_type = target_type , ) if self . is_masked : # Also downsample mask self . mask = downsample_dataframe ( self . mask , sampling_freq = sampling_freq , n_samples = n_samples , target_type = target_type , ) # Ensure mask stays boolean self . mask . loc [:, :] = self . mask > 0 # Masked data self . masked_data = downsample_dataframe ( self . masked_data , sampling_freq = sampling_freq , n_samples = n_samples , target_type = target_type , ) # Dilated mask if self . is_mask_dilated : self . dilated_mask = downsample_dataframe ( self . dilated_mask , sampling_freq = sampling_freq , n_samples = n_samples , target_type = target_type , ) # Ensure mask stays boolean self . dilated_mask . loc [:, :] = self . dilated_mask > 0 if self . is_fit : self . predictions = downsample_dataframe ( self . predictions , sampling_freq = sampling_freq , n_samples = n_samples , target_type = target_type , ) fit ( self , ** kwargs ) Replaced by sub-classes. This call just ensures that a model's data is sparse prior to fitting Source code in emotioncf/base.py def fit ( self , ** kwargs ): \"\"\"Replaced by sub-classes. This call just ensures that a model's data is sparse prior to fitting\"\"\" if not self . is_masked : raise ValueError ( \"You're trying to fit on a dense matrix, because model data has not been masked! Either call the `.create_masked_data` method prior to fitting or re-initialize the model and set the `mask` or `n_mask_items` arguments.\" ) if kwargs . get ( \"dilate_by_nsamples\" , None ) and self . is_mask_dilated : warnings . warn ( \".fit() was called with dilate_by_nsamples=None, but model mask is already dilated! This will undo dilation and then fit a model. Instead pass dilate_by_nsamples, directly to .fit()\" ) plot_predictions ( self , dataset = 'missing' , verbose = True , figsize = ( 15 , 8 ), heatmapkwargs = {}) Create plot of actual vs predicted values. Parameters: Name Type Description Default dataset str; optional one of 'full', 'observed', or 'missing'. Default 'missing'. 'missing' verbose bool; optional print the averaged subject correlation while plotting; Default True True Returns: Type Description r (float) Correlation Source code in emotioncf/base.py def plot_predictions ( self , dataset = \"missing\" , verbose = True , figsize = ( 15 , 8 ), heatmapkwargs = {} ): \"\"\"Create plot of actual vs predicted values. Args: dataset (str; optional): one of 'full', 'observed', or 'missing'. Default 'missing'. verbose (bool; optional): print the averaged subject correlation while plotting; Default True Returns: r (float): Correlation \"\"\" if not self . is_fit : raise ValueError ( \"Model has not been fit\" ) vmax = max ( self . masked_data . max () . max (), self . predictions . max () . max ()) vmin = min ( self . masked_data . min () . min (), self . predictions . min () . min ()) actual , pred = self . _retrieve_predictions ( dataset ) if actual is None : ncols = 2 if verbose : warnings . warn ( \"Cannot score predictions on missing data because true values were never observed!\" ) else : ncols = 3 heatmapkwargs . setdefault ( \"square\" , False ) heatmapkwargs . setdefault ( \"xticklabels\" , False ) heatmapkwargs . setdefault ( \"yticklabels\" , False ) heatmapkwargs . setdefault ( \"vmax\" , vmax ) heatmapkwargs . setdefault ( \"vmin\" , vmin ) f , ax = plt . subplots ( nrows = 1 , ncols = ncols , figsize = figsize ) # The original data matrix (potentially masked) sns . heatmap ( self . masked_data , ax = ax [ 0 ], ** heatmapkwargs ) ax [ 0 ] . set_title ( \"Actual User/Item Ratings\" ) ax [ 0 ] . set_xlabel ( \"Items\" , fontsize = 18 ) ax [ 0 ] . set_ylabel ( \"Users\" , fontsize = 18 ) # The predicted data matrix sns . heatmap ( self . predictions , ax = ax [ 1 ], ** heatmapkwargs ) ax [ 1 ] . set_title ( \"Predicted User/Item Ratings\" ) ax [ 1 ] . set_xlabel ( \"Items\" , fontsize = 18 ) ax [ 1 ] . set_ylabel ( \"Users\" , fontsize = 18 ) f . tight_layout () # Scatter plot if we can calculate it if actual is not None : nans = np . logical_or ( np . isnan ( actual ), np . isnan ( pred )) ax [ 2 ] . scatter ( actual [ ~ nans ], pred [ ~ nans ], ) ax [ 2 ] . set_xlabel ( \"Actual Ratings\" ) ax [ 2 ] . set_ylabel ( \"Predicted Ratings\" ) ax [ 2 ] . set_title ( \"Predicted Ratings\" ) r = self . score ( dataset = dataset , by_subject = True , metric = \"correlation\" ) . mean () if verbose : print ( \"Average Subject Correlation: %s \" % r ) return f , r return f score ( self , metric = 'rmse' , by_subject = False , dataset = 'missing' , verbose = True ) Get the performance of a fitted model by comparing observed and predicted data. This method is primarily useful if you want to calculate a single metric. Otherwise you should prefer the .summary() method instead, which scores all metrics. Parameters: Name Type Description Default metric str; optional what metric to compute, one of 'rmse', 'mse', 'mae' or 'correlation'; Default 'rmse'. 'rmse' dataset str; optional how to compute scoring, either using 'observed', 'missing' or 'full'. Default 'missing'. 'missing' Returns: Type Description float score Source code in emotioncf/base.py def score ( self , metric = \"rmse\" , by_subject = False , dataset = \"missing\" , verbose = True ): \"\"\"Get the performance of a fitted model by comparing observed and predicted data. This method is primarily useful if you want to calculate a single metric. Otherwise you should prefer the `.summary()` method instead, which scores all metrics. Args: metric (str; optional): what metric to compute, one of 'rmse', 'mse', 'mae' or 'correlation'; Default 'rmse'. dataset (str; optional): how to compute scoring, either using 'observed', 'missing' or 'full'. Default 'missing'. Returns: float: score \"\"\" if not self . is_fit : raise ValueError ( \"You must fit() model first before using this method.\" ) if metric not in [ \"rmse\" , \"mse\" , \"mae\" , \"correlation\" ]: raise ValueError ( \"metric must be one of 'rmse', 'mse', 'mae', or 'correlation'\" ) # Get dataframes of observed and predicted values # This will be a dense or sparse matrix the same shape as the input data actual , pred = self . _retrieve_predictions ( dataset ) if actual is None : if verbose : warnings . warn ( \"Cannot score predictions on missing data because true values were never observed!\" ) return None if by_subject : scores = [] for userid in range ( actual . shape [ 0 ]): user_actual = actual . iloc [ userid , :] . values user_pred = pred . iloc [ userid , :] . values if metric == \"rmse\" : score = np . sqrt ( np . nanmean (( user_pred - user_actual ) ** 2 )) elif metric == \"mse\" : score = np . nanmean (( user_pred - user_actual ) ** 2 ) elif metric == \"mae\" : score = np . nanmean ( np . abs ( user_pred - user_actual )) elif metric == \"correlation\" : nans = np . logical_or ( np . isnan ( user_actual ), np . isnan ( user_pred )) if len ( user_actual [ ~ nans ]) < 2 or len ( user_pred [ ~ nans ]) < 2 : score = np . nan else : score = pearsonr ( user_actual [ ~ nans ], user_pred [ ~ nans ])[ 0 ] scores . append ( score ) return pd . Series ( scores , index = actual . index , name = f \" { metric } _ { dataset } \" ) else : actual , pred = actual . to_numpy () . flatten (), pred . to_numpy () . flatten () if metric == \"rmse\" : return np . sqrt ( np . nanmean (( pred - actual ) ** 2 )) elif metric == \"mse\" : return np . nanmean (( pred - actual ) ** 2 ) elif metric == \"mae\" : return np . nanmean ( np . abs ( pred - actual )) elif metric == \"correlation\" : nans = np . logical_or ( np . isnan ( actual ), np . isnan ( pred )) if len ( actual [ ~ nans ]) < 2 or len ( pred [ ~ nans ]) < 2 : return np . nan else : return pearsonr ( actual [ ~ nans ], pred [ ~ nans ])[ 0 ] summary ( self , verbose = True , return_cached = True ) Calculate the performance of a model and return a dataframe of results. Computes performance across all, observed, and missing datasets. Scores using rmse, mse, mae, and correlation. Computes scores across all subjects (i.e. ignoring the fact that ratings are clustered by subject) and the mean performance for each metric after calculating per-subject performance. Parameters: Name Type Description Default verbose bool Print warning messages during scoring. Defaults to True. True return_cached bool Save time by returning already computed scores if they exist. Defaults to True. True Returns: Type Description pd.DataFrame long-form dataframe of model performance Source code in emotioncf/base.py def summary ( self , verbose = True , return_cached = True ): \"\"\" Calculate the performance of a model and return a dataframe of results. Computes performance across all, observed, and missing datasets. Scores using rmse, mse, mae, and correlation. Computes scores across all subjects (i.e. ignoring the fact that ratings are clustered by subject) and the mean performance for each metric after calculating per-subject performance. Args: verbose (bool, optional): Print warning messages during scoring. Defaults to True. return_cached (bool, optional): Save time by returning already computed scores if they exist. Defaults to True. Returns: pd.DataFrame: long-form dataframe of model performance \"\"\" if not self . is_fit : raise ValueError ( \"Model has not been fit!\" ) # Don't recompute results if we already have them if return_cached and self . results is not None : if verbose : print ( \"Returning cached scores...set cache=False to force recomputation\" ) return self . results # Compute results for all metrics, all datasets, separately for group and by subject group_results = { \"algorithm\" : self . __class__ . __name__ , } subject_results = [] with warnings . catch_warnings ( record = True ) as w : warnings . simplefilter ( \"always\" ) for metric in [ \"rmse\" , \"mse\" , \"mae\" , \"correlation\" ]: this_group_result = {} this_subject_result = [] for dataset in [ \"full\" , \"missing\" , \"observed\" ]: this_group_result [ dataset ] = self . score ( metric = metric , dataset = dataset , verbose = verbose ) this_subject_result . append ( self . score ( metric = metric , dataset = dataset , by_subject = True , verbose = verbose , ) ) # Dict of group results for this metric group_results [ metric ] = this_group_result # Dataframe of subject results for this metric this_subject_result = pd . concat ( this_subject_result , axis = 1 ) subject_results . append ( this_subject_result ) group_results [ f \" { metric } _subject\" ] = dict ( zip ( [ \"full\" , \"missing\" , \"observed\" ], this_subject_result . mean () . values , ) ) # Save final results to longform df self . subject_results = pd . concat ( subject_results , axis = 1 ) group_results = pd . DataFrame ( group_results ) group_results = ( group_results . reset_index () . melt ( id_vars = [ \"index\" , \"algorithm\" ], var_name = \"metric\" , value_name = \"score\" , ) . rename ( columns = { \"index\" : \"dataset\" }) . sort_values ( by = [ \"dataset\" , \"metric\" ]) . reset_index ( drop = True ) . assign ( group = lambda df : df . metric . apply ( lambda x : \"subject\" if \"subject\" in x else \"all\" ), metric = lambda df : df . metric . replace ( { \"correlation_subject\" : \"correlation\" , \"mse_subject\" : \"mse\" , \"rmse_subject\" : \"rmse\" , \"mae_subject\" : \"mae\" , } ), ) . sort_values ( by = [ \"dataset\" , \"metric\" , \"group\" ]) . reset_index ( drop = True )[ [ \"algorithm\" , \"dataset\" , \"group\" , \"metric\" , \"score\" ] ] ) self . results = group_results if verbose and w : print ( w [ - 1 ] . message ) return self . results to_long_df ( self ) Create a long format pandas dataframe with observed, predicted, and mask. Source code in emotioncf/base.py def to_long_df ( self ): \"\"\" Create a long format pandas dataframe with observed, predicted, and mask.\"\"\" observed = pd . DataFrame ( columns = [ \"Subject\" , \"Item\" , \"Rating\" , \"Condition\" ]) for row in self . data . iterrows (): tmp = pd . DataFrame ( columns = observed . columns ) tmp [ \"Rating\" ] = row [ 1 ] tmp [ \"Item\" ] = self . data . columns tmp [ \"Subject\" ] = row [ 0 ] tmp [ \"Condition\" ] = \"Observed\" if self . is_masked : if self . is_mask_dilated : tmp [ \"Mask\" ] = self . dilated_mask . loc [ row [ 0 ]] else : tmp [ \"Mask\" ] = self . mask . loc [ row [ 0 ]] observed = observed . append ( tmp ) if self . is_fit : predicted = pd . DataFrame ( columns = [ \"Subject\" , \"Item\" , \"Rating\" , \"Condition\" ]) for row in self . predictions . iterrows (): tmp = pd . DataFrame ( columns = predicted . columns ) tmp [ \"Rating\" ] = row [ 1 ] tmp [ \"Item\" ] = self . predictions . columns tmp [ \"Subject\" ] = row [ 0 ] tmp [ \"Condition\" ] = \"Predicted\" if self . is_masked : tmp [ \"Mask\" ] = self . mask . loc [ row [ 0 ]] predicted = predicted . append ( tmp ) observed = observed . append ( predicted ) return observed BaseNMF Base class for NMF algorithms. plot_learning ( self , save = False ) Plot training error over iterations for diagnostic purposes Parameters: Name Type Description Default save bool/str/Path if a string or path is provided will save the figure to that location. Defaults to False. False Returns: Type Description tuple (figure handle, axes handle) Source code in emotioncf/base.py def plot_learning ( self , save = False ): \"\"\" Plot training error over iterations for diagnostic purposes Args: save (bool/str/Path, optional): if a string or path is provided will save the figure to that location. Defaults to False. Returns: tuple: (figure handle, axes handle) \"\"\" if self . is_fit : f , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 6 )) _ = ax . plot ( range ( 1 , len ( self . error_history ) + 1 ), self . error_history ) ax . set ( xlabel = \"Iteration\" , ylabel = \"Normalized RMSE\" , title = f \"Final Normalized RMSE: { np . round ( self . _norm_rmse , 3 ) } \\n Converged: { self . converged } \" , ) if save : plt . savefig ( save , bbox_inches = \"tight\" ) return f , ax else : raise ValueError ( \"Model has not been fit.\" )","title":"Base"},{"location":"api/base/#emotioncfbase","text":"Base algorithm classes. All other algorithms inherit from these classes which means they have access to all their methods and attributes. You won't typically utilize these classes directly unless you're creating a custom estimator.","title":"emotioncf.base"},{"location":"api/base/#emotioncf.base.Base","text":"All other models and base classes inherit from this class.","title":"Base"},{"location":"api/base/#emotioncf.base.Base.create_masked_data","text":"Create a mask and apply it to data using number of items or % of items Parameters: Name Type Description Default n_itmes int/float if an integer is passed its raw value is used. Otherwise if a float is passed its taken to be a (rounded) percentage of the total items; Default .1 (10% of the data) required Source code in emotioncf/base.py def create_masked_data ( self , n_mask_items = 0.1 ): \"\"\" Create a mask and apply it to data using number of items or % of items Args: n_itmes (int/float, optional): if an integer is passed its raw value is used. Otherwise if a float is passed its taken to be a (rounded) percentage of the total items; Default .1 (10% of the data) \"\"\" if ( isinstance ( n_mask_items , np . floating ) and ( n_mask_items >= 1.0 or n_mask_items <= 0.0 ) ) or ( isinstance ( n_mask_items , int ) and ( n_mask_items >= self . data . shape [ 1 ] or n_mask_items <= 0 ) ): raise TypeError ( \"n_items should a float between 0-1 or an integer < the number of items\" ) self . mask = create_train_test_mask ( self . data , n_mask_items ) self . masked_data = self . data [ self . mask ] self . is_masked = True self . n_mask_items = n_mask_items","title":"create_masked_data()"},{"location":"api/base/#emotioncf.base.Base.dilate_mask","text":"Dilate sparse time-series data by n_samples. Overlapping data will be averaged. This method computes and stores the dilated mask in .dilated_mask and internally updates the .masked_data . Repeated calls to this method on the same model instance do not stack, but rather perform a new dilation on the original masked data. Called this method with None will undo any dilation. This is an alias to ._dilate_ts_rating_samples Parameters: Name Type Description Default nsamples int Number of samples to dilate data required Source code in emotioncf/base.py def dilate_mask ( self , n_samples = None ): \"\"\"Dilate sparse time-series data by n_samples. Overlapping data will be averaged. This method computes and stores the dilated mask in `.dilated_mask` and internally updates the `.masked_data`. Repeated calls to this method on the same model instance **do not** stack, but rather perform a new dilation on the original masked data. Called this method with `None` will undo any dilation. This is an alias to `._dilate_ts_rating_samples` Args: nsamples (int): Number of samples to dilate data \"\"\" self . _dilate_ts_rating_samples ( n_samples = n_samples )","title":"dilate_mask()"},{"location":"api/base/#emotioncf.base.Base.downsample","text":"Downsample a model's rating matrix to a new target frequency or number of samples using averaging. Also downsamples a model's mask and dilated mask if they exist as well as a model's predictions if it's already been fit. If target_type = 'samples' and sampling_freq is None, the new user x item matrix will have shape users x items * (1 / n_samples). If target_type = 'seconds', the new user x item matrix will have shape users x items * (1 / n_samples * sampling_freq). If target_type = 'hz', the new user x item matrix will have shape users x items * (1 / sampling_freq / n_samples). Parameters: Name Type Description Default n_samples int number of samples required sampling_freq int/float Sampling frequency of data; Default None None target_type str how to downsample; must be one of \"samples\", \"seconds\" or \"hz\". Defaults to \"samples\". 'samples' Source code in emotioncf/base.py def downsample ( self , n_samples , sampling_freq = None , target_type = \"samples\" ): \"\"\" Downsample a model's rating matrix to a new target frequency or number of samples using averaging. Also downsamples a model's mask and dilated mask if they exist as well as a model's predictions if it's already been fit. If target_type = 'samples' and sampling_freq is None, the new user x item matrix will have shape users x items * (1 / n_samples). If target_type = 'seconds', the new user x item matrix will have shape users x items * (1 / n_samples * sampling_freq). If target_type = 'hz', the new user x item matrix will have shape users x items * (1 / sampling_freq / n_samples). Args: n_samples (int): number of samples sampling_freq (int/float): Sampling frequency of data; Default None target_type (str, optional): how to downsample; must be one of \"samples\", \"seconds\" or \"hz\". Defaults to \"samples\". \"\"\" self . data = downsample_dataframe ( self . data , sampling_freq = sampling_freq , n_samples = n_samples , target_type = target_type , ) if self . is_masked : # Also downsample mask self . mask = downsample_dataframe ( self . mask , sampling_freq = sampling_freq , n_samples = n_samples , target_type = target_type , ) # Ensure mask stays boolean self . mask . loc [:, :] = self . mask > 0 # Masked data self . masked_data = downsample_dataframe ( self . masked_data , sampling_freq = sampling_freq , n_samples = n_samples , target_type = target_type , ) # Dilated mask if self . is_mask_dilated : self . dilated_mask = downsample_dataframe ( self . dilated_mask , sampling_freq = sampling_freq , n_samples = n_samples , target_type = target_type , ) # Ensure mask stays boolean self . dilated_mask . loc [:, :] = self . dilated_mask > 0 if self . is_fit : self . predictions = downsample_dataframe ( self . predictions , sampling_freq = sampling_freq , n_samples = n_samples , target_type = target_type , )","title":"downsample()"},{"location":"api/base/#emotioncf.base.Base.fit","text":"Replaced by sub-classes. This call just ensures that a model's data is sparse prior to fitting Source code in emotioncf/base.py def fit ( self , ** kwargs ): \"\"\"Replaced by sub-classes. This call just ensures that a model's data is sparse prior to fitting\"\"\" if not self . is_masked : raise ValueError ( \"You're trying to fit on a dense matrix, because model data has not been masked! Either call the `.create_masked_data` method prior to fitting or re-initialize the model and set the `mask` or `n_mask_items` arguments.\" ) if kwargs . get ( \"dilate_by_nsamples\" , None ) and self . is_mask_dilated : warnings . warn ( \".fit() was called with dilate_by_nsamples=None, but model mask is already dilated! This will undo dilation and then fit a model. Instead pass dilate_by_nsamples, directly to .fit()\" )","title":"fit()"},{"location":"api/base/#emotioncf.base.Base.plot_predictions","text":"Create plot of actual vs predicted values. Parameters: Name Type Description Default dataset str; optional one of 'full', 'observed', or 'missing'. Default 'missing'. 'missing' verbose bool; optional print the averaged subject correlation while plotting; Default True True Returns: Type Description r (float) Correlation Source code in emotioncf/base.py def plot_predictions ( self , dataset = \"missing\" , verbose = True , figsize = ( 15 , 8 ), heatmapkwargs = {} ): \"\"\"Create plot of actual vs predicted values. Args: dataset (str; optional): one of 'full', 'observed', or 'missing'. Default 'missing'. verbose (bool; optional): print the averaged subject correlation while plotting; Default True Returns: r (float): Correlation \"\"\" if not self . is_fit : raise ValueError ( \"Model has not been fit\" ) vmax = max ( self . masked_data . max () . max (), self . predictions . max () . max ()) vmin = min ( self . masked_data . min () . min (), self . predictions . min () . min ()) actual , pred = self . _retrieve_predictions ( dataset ) if actual is None : ncols = 2 if verbose : warnings . warn ( \"Cannot score predictions on missing data because true values were never observed!\" ) else : ncols = 3 heatmapkwargs . setdefault ( \"square\" , False ) heatmapkwargs . setdefault ( \"xticklabels\" , False ) heatmapkwargs . setdefault ( \"yticklabels\" , False ) heatmapkwargs . setdefault ( \"vmax\" , vmax ) heatmapkwargs . setdefault ( \"vmin\" , vmin ) f , ax = plt . subplots ( nrows = 1 , ncols = ncols , figsize = figsize ) # The original data matrix (potentially masked) sns . heatmap ( self . masked_data , ax = ax [ 0 ], ** heatmapkwargs ) ax [ 0 ] . set_title ( \"Actual User/Item Ratings\" ) ax [ 0 ] . set_xlabel ( \"Items\" , fontsize = 18 ) ax [ 0 ] . set_ylabel ( \"Users\" , fontsize = 18 ) # The predicted data matrix sns . heatmap ( self . predictions , ax = ax [ 1 ], ** heatmapkwargs ) ax [ 1 ] . set_title ( \"Predicted User/Item Ratings\" ) ax [ 1 ] . set_xlabel ( \"Items\" , fontsize = 18 ) ax [ 1 ] . set_ylabel ( \"Users\" , fontsize = 18 ) f . tight_layout () # Scatter plot if we can calculate it if actual is not None : nans = np . logical_or ( np . isnan ( actual ), np . isnan ( pred )) ax [ 2 ] . scatter ( actual [ ~ nans ], pred [ ~ nans ], ) ax [ 2 ] . set_xlabel ( \"Actual Ratings\" ) ax [ 2 ] . set_ylabel ( \"Predicted Ratings\" ) ax [ 2 ] . set_title ( \"Predicted Ratings\" ) r = self . score ( dataset = dataset , by_subject = True , metric = \"correlation\" ) . mean () if verbose : print ( \"Average Subject Correlation: %s \" % r ) return f , r return f","title":"plot_predictions()"},{"location":"api/base/#emotioncf.base.Base.score","text":"Get the performance of a fitted model by comparing observed and predicted data. This method is primarily useful if you want to calculate a single metric. Otherwise you should prefer the .summary() method instead, which scores all metrics. Parameters: Name Type Description Default metric str; optional what metric to compute, one of 'rmse', 'mse', 'mae' or 'correlation'; Default 'rmse'. 'rmse' dataset str; optional how to compute scoring, either using 'observed', 'missing' or 'full'. Default 'missing'. 'missing' Returns: Type Description float score Source code in emotioncf/base.py def score ( self , metric = \"rmse\" , by_subject = False , dataset = \"missing\" , verbose = True ): \"\"\"Get the performance of a fitted model by comparing observed and predicted data. This method is primarily useful if you want to calculate a single metric. Otherwise you should prefer the `.summary()` method instead, which scores all metrics. Args: metric (str; optional): what metric to compute, one of 'rmse', 'mse', 'mae' or 'correlation'; Default 'rmse'. dataset (str; optional): how to compute scoring, either using 'observed', 'missing' or 'full'. Default 'missing'. Returns: float: score \"\"\" if not self . is_fit : raise ValueError ( \"You must fit() model first before using this method.\" ) if metric not in [ \"rmse\" , \"mse\" , \"mae\" , \"correlation\" ]: raise ValueError ( \"metric must be one of 'rmse', 'mse', 'mae', or 'correlation'\" ) # Get dataframes of observed and predicted values # This will be a dense or sparse matrix the same shape as the input data actual , pred = self . _retrieve_predictions ( dataset ) if actual is None : if verbose : warnings . warn ( \"Cannot score predictions on missing data because true values were never observed!\" ) return None if by_subject : scores = [] for userid in range ( actual . shape [ 0 ]): user_actual = actual . iloc [ userid , :] . values user_pred = pred . iloc [ userid , :] . values if metric == \"rmse\" : score = np . sqrt ( np . nanmean (( user_pred - user_actual ) ** 2 )) elif metric == \"mse\" : score = np . nanmean (( user_pred - user_actual ) ** 2 ) elif metric == \"mae\" : score = np . nanmean ( np . abs ( user_pred - user_actual )) elif metric == \"correlation\" : nans = np . logical_or ( np . isnan ( user_actual ), np . isnan ( user_pred )) if len ( user_actual [ ~ nans ]) < 2 or len ( user_pred [ ~ nans ]) < 2 : score = np . nan else : score = pearsonr ( user_actual [ ~ nans ], user_pred [ ~ nans ])[ 0 ] scores . append ( score ) return pd . Series ( scores , index = actual . index , name = f \" { metric } _ { dataset } \" ) else : actual , pred = actual . to_numpy () . flatten (), pred . to_numpy () . flatten () if metric == \"rmse\" : return np . sqrt ( np . nanmean (( pred - actual ) ** 2 )) elif metric == \"mse\" : return np . nanmean (( pred - actual ) ** 2 ) elif metric == \"mae\" : return np . nanmean ( np . abs ( pred - actual )) elif metric == \"correlation\" : nans = np . logical_or ( np . isnan ( actual ), np . isnan ( pred )) if len ( actual [ ~ nans ]) < 2 or len ( pred [ ~ nans ]) < 2 : return np . nan else : return pearsonr ( actual [ ~ nans ], pred [ ~ nans ])[ 0 ]","title":"score()"},{"location":"api/base/#emotioncf.base.Base.summary","text":"Calculate the performance of a model and return a dataframe of results. Computes performance across all, observed, and missing datasets. Scores using rmse, mse, mae, and correlation. Computes scores across all subjects (i.e. ignoring the fact that ratings are clustered by subject) and the mean performance for each metric after calculating per-subject performance. Parameters: Name Type Description Default verbose bool Print warning messages during scoring. Defaults to True. True return_cached bool Save time by returning already computed scores if they exist. Defaults to True. True Returns: Type Description pd.DataFrame long-form dataframe of model performance Source code in emotioncf/base.py def summary ( self , verbose = True , return_cached = True ): \"\"\" Calculate the performance of a model and return a dataframe of results. Computes performance across all, observed, and missing datasets. Scores using rmse, mse, mae, and correlation. Computes scores across all subjects (i.e. ignoring the fact that ratings are clustered by subject) and the mean performance for each metric after calculating per-subject performance. Args: verbose (bool, optional): Print warning messages during scoring. Defaults to True. return_cached (bool, optional): Save time by returning already computed scores if they exist. Defaults to True. Returns: pd.DataFrame: long-form dataframe of model performance \"\"\" if not self . is_fit : raise ValueError ( \"Model has not been fit!\" ) # Don't recompute results if we already have them if return_cached and self . results is not None : if verbose : print ( \"Returning cached scores...set cache=False to force recomputation\" ) return self . results # Compute results for all metrics, all datasets, separately for group and by subject group_results = { \"algorithm\" : self . __class__ . __name__ , } subject_results = [] with warnings . catch_warnings ( record = True ) as w : warnings . simplefilter ( \"always\" ) for metric in [ \"rmse\" , \"mse\" , \"mae\" , \"correlation\" ]: this_group_result = {} this_subject_result = [] for dataset in [ \"full\" , \"missing\" , \"observed\" ]: this_group_result [ dataset ] = self . score ( metric = metric , dataset = dataset , verbose = verbose ) this_subject_result . append ( self . score ( metric = metric , dataset = dataset , by_subject = True , verbose = verbose , ) ) # Dict of group results for this metric group_results [ metric ] = this_group_result # Dataframe of subject results for this metric this_subject_result = pd . concat ( this_subject_result , axis = 1 ) subject_results . append ( this_subject_result ) group_results [ f \" { metric } _subject\" ] = dict ( zip ( [ \"full\" , \"missing\" , \"observed\" ], this_subject_result . mean () . values , ) ) # Save final results to longform df self . subject_results = pd . concat ( subject_results , axis = 1 ) group_results = pd . DataFrame ( group_results ) group_results = ( group_results . reset_index () . melt ( id_vars = [ \"index\" , \"algorithm\" ], var_name = \"metric\" , value_name = \"score\" , ) . rename ( columns = { \"index\" : \"dataset\" }) . sort_values ( by = [ \"dataset\" , \"metric\" ]) . reset_index ( drop = True ) . assign ( group = lambda df : df . metric . apply ( lambda x : \"subject\" if \"subject\" in x else \"all\" ), metric = lambda df : df . metric . replace ( { \"correlation_subject\" : \"correlation\" , \"mse_subject\" : \"mse\" , \"rmse_subject\" : \"rmse\" , \"mae_subject\" : \"mae\" , } ), ) . sort_values ( by = [ \"dataset\" , \"metric\" , \"group\" ]) . reset_index ( drop = True )[ [ \"algorithm\" , \"dataset\" , \"group\" , \"metric\" , \"score\" ] ] ) self . results = group_results if verbose and w : print ( w [ - 1 ] . message ) return self . results","title":"summary()"},{"location":"api/base/#emotioncf.base.Base.to_long_df","text":"Create a long format pandas dataframe with observed, predicted, and mask. Source code in emotioncf/base.py def to_long_df ( self ): \"\"\" Create a long format pandas dataframe with observed, predicted, and mask.\"\"\" observed = pd . DataFrame ( columns = [ \"Subject\" , \"Item\" , \"Rating\" , \"Condition\" ]) for row in self . data . iterrows (): tmp = pd . DataFrame ( columns = observed . columns ) tmp [ \"Rating\" ] = row [ 1 ] tmp [ \"Item\" ] = self . data . columns tmp [ \"Subject\" ] = row [ 0 ] tmp [ \"Condition\" ] = \"Observed\" if self . is_masked : if self . is_mask_dilated : tmp [ \"Mask\" ] = self . dilated_mask . loc [ row [ 0 ]] else : tmp [ \"Mask\" ] = self . mask . loc [ row [ 0 ]] observed = observed . append ( tmp ) if self . is_fit : predicted = pd . DataFrame ( columns = [ \"Subject\" , \"Item\" , \"Rating\" , \"Condition\" ]) for row in self . predictions . iterrows (): tmp = pd . DataFrame ( columns = predicted . columns ) tmp [ \"Rating\" ] = row [ 1 ] tmp [ \"Item\" ] = self . predictions . columns tmp [ \"Subject\" ] = row [ 0 ] tmp [ \"Condition\" ] = \"Predicted\" if self . is_masked : tmp [ \"Mask\" ] = self . mask . loc [ row [ 0 ]] predicted = predicted . append ( tmp ) observed = observed . append ( predicted ) return observed","title":"to_long_df()"},{"location":"api/base/#emotioncf.base.BaseNMF","text":"Base class for NMF algorithms.","title":"BaseNMF"},{"location":"api/base/#emotioncf.base.BaseNMF.plot_learning","text":"Plot training error over iterations for diagnostic purposes Parameters: Name Type Description Default save bool/str/Path if a string or path is provided will save the figure to that location. Defaults to False. False Returns: Type Description tuple (figure handle, axes handle) Source code in emotioncf/base.py def plot_learning ( self , save = False ): \"\"\" Plot training error over iterations for diagnostic purposes Args: save (bool/str/Path, optional): if a string or path is provided will save the figure to that location. Defaults to False. Returns: tuple: (figure handle, axes handle) \"\"\" if self . is_fit : f , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 6 )) _ = ax . plot ( range ( 1 , len ( self . error_history ) + 1 ), self . error_history ) ax . set ( xlabel = \"Iteration\" , ylabel = \"Normalized RMSE\" , title = f \"Final Normalized RMSE: { np . round ( self . _norm_rmse , 3 ) } \\n Converged: { self . converged } \" , ) if save : plt . savefig ( save , bbox_inches = \"tight\" ) return f , ax else : raise ValueError ( \"Model has not been fit.\" )","title":"plot_learning()"},{"location":"api/knn/","text":"emotioncf.models.KNN The K-Nearest Neighbors algorithm makes predictions using a weighted mean of a subset of similar users. Similarity can be controlled via the metric argument to the .fit method, and the number of other users can be controlled with the k argument to the .predict method. fit ( self , k = None , metric = 'pearson' , dilate_by_nsamples = None , skip_refit = False , ** kwargs ) Fit collaborative model to train data. Calculate similarity between subjects across items. Repeated called to fit with different k, but the same previous arguments will re-use the computed user x user similarity matrix. Parameters: Name Type Description Default k int number of closest neighbors to use None metric str; optional type of similarity. One of 'pearson', 'spearman', 'kendall', 'cosine', or 'correlation'. 'correlation' is just an alias for 'pearson'. Default 'pearson'. 'pearson' skip_refit bool; optional skip re-estimation of user x user similarity matrix. Faster if only exploring different k and no other model parameters or masks are changing. Default False. False Source code in emotioncf/models.py def fit ( self , k = None , metric = \"pearson\" , dilate_by_nsamples = None , skip_refit = False , ** kwargs , ): \"\"\"Fit collaborative model to train data. Calculate similarity between subjects across items. Repeated called to fit with different k, but the same previous arguments will re-use the computed user x user similarity matrix. Args: k (int): number of closest neighbors to use metric (str; optional): type of similarity. One of 'pearson', 'spearman', 'kendall', 'cosine', or 'correlation'. 'correlation' is just an alias for 'pearson'. Default 'pearson'. skip_refit (bool; optional): skip re-estimation of user x user similarity matrix. Faster if only exploring different k and no other model parameters or masks are changing. Default False. \"\"\" metrics = [ \"pearson\" , \"spearman\" , \"kendall\" , \"cosine\" , \"correlation\" ] if metric not in metrics : raise ValueError ( f \"metric must be one of { metrics } \" ) # Call parent fit which acts as a guard for non-masked data super () . fit () # If fit is being called more than once in a row with different k, but no other arguments are changing, reuse the last computed similarity matrix to save time. Otherwise re-calculate it if not skip_refit : self . dilate_mask ( n_samples = dilate_by_nsamples ) if metric in [ \"pearson\" , \"kendall\" , \"spearman\" ]: # Fall back to pandas sim = self . masked_data . T . corr ( method = metric ) else : sim = pd . DataFrame ( 1 - nanpdist ( self . masked_data . to_numpy (), metric = metric ), index = self . masked_data . index , columns = self . masked_data . index , ) self . subject_similarity = sim self . _predict ( k = k ) self . is_fit = True","title":"KNN"},{"location":"api/knn/#emotioncfmodelsknn","text":"The K-Nearest Neighbors algorithm makes predictions using a weighted mean of a subset of similar users. Similarity can be controlled via the metric argument to the .fit method, and the number of other users can be controlled with the k argument to the .predict method.","title":"emotioncf.models.KNN"},{"location":"api/knn/#emotioncf.models.KNN.fit","text":"Fit collaborative model to train data. Calculate similarity between subjects across items. Repeated called to fit with different k, but the same previous arguments will re-use the computed user x user similarity matrix. Parameters: Name Type Description Default k int number of closest neighbors to use None metric str; optional type of similarity. One of 'pearson', 'spearman', 'kendall', 'cosine', or 'correlation'. 'correlation' is just an alias for 'pearson'. Default 'pearson'. 'pearson' skip_refit bool; optional skip re-estimation of user x user similarity matrix. Faster if only exploring different k and no other model parameters or masks are changing. Default False. False Source code in emotioncf/models.py def fit ( self , k = None , metric = \"pearson\" , dilate_by_nsamples = None , skip_refit = False , ** kwargs , ): \"\"\"Fit collaborative model to train data. Calculate similarity between subjects across items. Repeated called to fit with different k, but the same previous arguments will re-use the computed user x user similarity matrix. Args: k (int): number of closest neighbors to use metric (str; optional): type of similarity. One of 'pearson', 'spearman', 'kendall', 'cosine', or 'correlation'. 'correlation' is just an alias for 'pearson'. Default 'pearson'. skip_refit (bool; optional): skip re-estimation of user x user similarity matrix. Faster if only exploring different k and no other model parameters or masks are changing. Default False. \"\"\" metrics = [ \"pearson\" , \"spearman\" , \"kendall\" , \"cosine\" , \"correlation\" ] if metric not in metrics : raise ValueError ( f \"metric must be one of { metrics } \" ) # Call parent fit which acts as a guard for non-masked data super () . fit () # If fit is being called more than once in a row with different k, but no other arguments are changing, reuse the last computed similarity matrix to save time. Otherwise re-calculate it if not skip_refit : self . dilate_mask ( n_samples = dilate_by_nsamples ) if metric in [ \"pearson\" , \"kendall\" , \"spearman\" ]: # Fall back to pandas sim = self . masked_data . T . corr ( method = metric ) else : sim = pd . DataFrame ( 1 - nanpdist ( self . masked_data . to_numpy (), metric = metric ), index = self . masked_data . index , columns = self . masked_data . index , ) self . subject_similarity = sim self . _predict ( k = k ) self . is_fit = True","title":"fit()"},{"location":"api/mean/","text":"emotioncf.models.Mean The Mean algorithm simply uses the mean of other users to make predictions about items. It's primarily useful as a good baseline model. fit ( self , dilate_by_nsamples = None , ** kwargs ) Fit collaborative model to train data. Calculate similarity between subjects across items Parameters: Name Type Description Default dilate_ts_n_samples int will dilate masked samples by n_samples to leverage auto-correlation in estimating time-series data required Source code in emotioncf/models.py def fit ( self , dilate_by_nsamples = None , ** kwargs ): \"\"\"Fit collaborative model to train data. Calculate similarity between subjects across items Args: dilate_ts_n_samples (int): will dilate masked samples by n_samples to leverage auto-correlation in estimating time-series data \"\"\" # Call parent fit which acts as a guard for non-masked data super () . fit () self . dilate_mask ( n_samples = dilate_by_nsamples ) self . mean = self . masked_data . mean ( skipna = True , axis = 0 ) self . _predict () self . is_fit = True","title":"Mean"},{"location":"api/mean/#emotioncfmodelsmean","text":"The Mean algorithm simply uses the mean of other users to make predictions about items. It's primarily useful as a good baseline model.","title":"emotioncf.models.Mean"},{"location":"api/mean/#emotioncf.models.Mean.fit","text":"Fit collaborative model to train data. Calculate similarity between subjects across items Parameters: Name Type Description Default dilate_ts_n_samples int will dilate masked samples by n_samples to leverage auto-correlation in estimating time-series data required Source code in emotioncf/models.py def fit ( self , dilate_by_nsamples = None , ** kwargs ): \"\"\"Fit collaborative model to train data. Calculate similarity between subjects across items Args: dilate_ts_n_samples (int): will dilate masked samples by n_samples to leverage auto-correlation in estimating time-series data \"\"\" # Call parent fit which acts as a guard for non-masked data super () . fit () self . dilate_mask ( n_samples = dilate_by_nsamples ) self . mean = self . masked_data . mean ( skipna = True , axis = 0 ) self . _predict () self . is_fit = True","title":"fit()"},{"location":"api/nmf_m/","text":"emotioncf.models.NNMF_mult The non-negative matrix factorization algorithm tries to decompose a users x items matrix into two additional matrices: users x factors and factors x items. Training is performed via multiplicative updating and continues until convergence or the maximum number of training iterations has been reached. Unlike the NNMF_sgd , this implementation takes no hyper-parameters and thus is simpler and faster to use, but less flexible, i.e. no regularization. The number of factors, convergence, and maximum iterations can be controlled with the n_factors , tol , and max_iterations arguments to the .fit method. By default the number of factors = the number items. The implementation here follows closely that of Lee & Seung, 2001 (eq 4): https://papers.nips.cc/paper/2000/file/f9d1152547c0bde01830b7e8bd60024c-Paper.pdf fit ( self , n_factors = None , n_iterations = 1000 , tol = 1e-06 , eps = 1e-06 , verbose = False , dilate_by_nsamples = None , ** kwargs ) Fit NNMF collaborative filtering model to train data using multiplicative updating. Given non-negative matrix V find non-negative factors W and H by minimizing ||V - WH||^2 . Parameters: Name Type Description Default n_factors int number of factors to learn. Defaults to None which includes all factors. None n_iterations int total number of training iterations if convergence is not achieved. Defaults to 5000. 1000 tol float Convergence criteria. Model is considered converged if the change in error during training < tol. Defaults to 0.001. 1e-06 eps float; optiona small value added to denominator of update rules to avoid divide-by-zero errors; Default 1e-6. 1e-06 verbose bool print information about training. Defaults to False. False dilate_ts_n_samples int How many items to dilate by prior to training. Defaults to None. required save_learning bool Save error for each training iteration for diagnostic purposes. Set this to False if memory is a limitation and the n_iterations is very large. Defaults to True. required Source code in emotioncf/models.py def fit ( self , n_factors = None , n_iterations = 1000 , tol = 1e-6 , eps = 1e-6 , verbose = False , dilate_by_nsamples = None , ** kwargs , ): \"\"\"Fit NNMF collaborative filtering model to train data using multiplicative updating. Given non-negative matrix `V` find non-negative factors `W` and `H` by minimizing `||V - WH||^2`. Args: n_factors (int, optional): number of factors to learn. Defaults to None which includes all factors. n_iterations (int, optional): total number of training iterations if convergence is not achieved. Defaults to 5000. tol (float, optional): Convergence criteria. Model is considered converged if the change in error during training < tol. Defaults to 0.001. eps (float; optiona): small value added to denominator of update rules to avoid divide-by-zero errors; Default 1e-6. verbose (bool, optional): print information about training. Defaults to False. dilate_ts_n_samples (int, optional): How many items to dilate by prior to training. Defaults to None. save_learning (bool, optional): Save error for each training iteration for diagnostic purposes. Set this to False if memory is a limitation and the n_iterations is very large. Defaults to True. \"\"\" # Call parent fit which acts as a guard for non-masked data super () . fit () n_users , n_items = self . data . shape if ( isinstance ( n_factors , int ) and n_factors >= n_items ) or isinstance ( n_factors , np . floating ): raise TypeError ( \"n_factors must be an integer < number of items\" ) if n_factors is None : n_factors = n_items self . n_factors = n_factors # Initialize W and H at non-negative scaled random values # We use random initialization scaled by the data, like sklearn: https://github.com/scikit-learn/scikit-learn/blob/95119c13af77c76e150b753485c662b7c52a41a2/sklearn/decomposition/_nmf.py#L334 avg = np . sqrt ( np . nanmean ( self . data ) / n_factors ) self . H = avg * np . random . rand ( n_factors , n_items ) self . W = avg * np . random . rand ( n_users , n_factors ) # Unlike SGD, we explicitly set missing data to 0 so that it gets ignored in the multiplicative update. See Zhu, 2016 for a justification of using a binary mask matrix: https://arxiv.org/pdf/1612.06037.pdf self . dilate_mask ( n_samples = dilate_by_nsamples ) # fillna(0) is equivalent to hadamard (element-wise) product with a binary mask X = self . masked_data . fillna ( 0 ) . to_numpy () # Run multiplicative updating error_history , converged , n_iter , delta , norm_rmse , W , H = mult ( X , self . W , self . H , self . data_range , eps , tol , n_iterations , verbose , ) # Save outputs to model self . W , self . H = W , H self . error_history = error_history self . _n_iter = n_iter self . _delta = delta self . _norm_rmse = norm_rmse self . converged = converged if verbose : if self . converged : print ( \" \\n\\t CONVERGED!\" ) print ( f \" \\n\\t Final Iteration: { self . _n_iter } \" ) print ( f \" \\t Final Delta: { np . round ( self . _delta ) } \" ) else : print ( \" \\t FAILED TO CONVERGE (n_iter reached)\" ) print ( f \" \\n\\t Final Iteration: { self . _n_iter } \" ) print ( f \" \\t Final delta exceeds tol: { tol } <= { np . round ( self . _delta , 5 ) } \" ) print ( f \" \\t Final Norm Error: { np . round ( 100 * norm_rmse , 2 ) } %\" ) self . _predict () self . is_fit = True","title":"NNMF_mult"},{"location":"api/nmf_m/#emotioncfmodelsnnmf_mult","text":"The non-negative matrix factorization algorithm tries to decompose a users x items matrix into two additional matrices: users x factors and factors x items. Training is performed via multiplicative updating and continues until convergence or the maximum number of training iterations has been reached. Unlike the NNMF_sgd , this implementation takes no hyper-parameters and thus is simpler and faster to use, but less flexible, i.e. no regularization. The number of factors, convergence, and maximum iterations can be controlled with the n_factors , tol , and max_iterations arguments to the .fit method. By default the number of factors = the number items. The implementation here follows closely that of Lee & Seung, 2001 (eq 4): https://papers.nips.cc/paper/2000/file/f9d1152547c0bde01830b7e8bd60024c-Paper.pdf","title":"emotioncf.models.NNMF_mult"},{"location":"api/nmf_m/#emotioncf.models.NNMF_mult.fit","text":"Fit NNMF collaborative filtering model to train data using multiplicative updating. Given non-negative matrix V find non-negative factors W and H by minimizing ||V - WH||^2 . Parameters: Name Type Description Default n_factors int number of factors to learn. Defaults to None which includes all factors. None n_iterations int total number of training iterations if convergence is not achieved. Defaults to 5000. 1000 tol float Convergence criteria. Model is considered converged if the change in error during training < tol. Defaults to 0.001. 1e-06 eps float; optiona small value added to denominator of update rules to avoid divide-by-zero errors; Default 1e-6. 1e-06 verbose bool print information about training. Defaults to False. False dilate_ts_n_samples int How many items to dilate by prior to training. Defaults to None. required save_learning bool Save error for each training iteration for diagnostic purposes. Set this to False if memory is a limitation and the n_iterations is very large. Defaults to True. required Source code in emotioncf/models.py def fit ( self , n_factors = None , n_iterations = 1000 , tol = 1e-6 , eps = 1e-6 , verbose = False , dilate_by_nsamples = None , ** kwargs , ): \"\"\"Fit NNMF collaborative filtering model to train data using multiplicative updating. Given non-negative matrix `V` find non-negative factors `W` and `H` by minimizing `||V - WH||^2`. Args: n_factors (int, optional): number of factors to learn. Defaults to None which includes all factors. n_iterations (int, optional): total number of training iterations if convergence is not achieved. Defaults to 5000. tol (float, optional): Convergence criteria. Model is considered converged if the change in error during training < tol. Defaults to 0.001. eps (float; optiona): small value added to denominator of update rules to avoid divide-by-zero errors; Default 1e-6. verbose (bool, optional): print information about training. Defaults to False. dilate_ts_n_samples (int, optional): How many items to dilate by prior to training. Defaults to None. save_learning (bool, optional): Save error for each training iteration for diagnostic purposes. Set this to False if memory is a limitation and the n_iterations is very large. Defaults to True. \"\"\" # Call parent fit which acts as a guard for non-masked data super () . fit () n_users , n_items = self . data . shape if ( isinstance ( n_factors , int ) and n_factors >= n_items ) or isinstance ( n_factors , np . floating ): raise TypeError ( \"n_factors must be an integer < number of items\" ) if n_factors is None : n_factors = n_items self . n_factors = n_factors # Initialize W and H at non-negative scaled random values # We use random initialization scaled by the data, like sklearn: https://github.com/scikit-learn/scikit-learn/blob/95119c13af77c76e150b753485c662b7c52a41a2/sklearn/decomposition/_nmf.py#L334 avg = np . sqrt ( np . nanmean ( self . data ) / n_factors ) self . H = avg * np . random . rand ( n_factors , n_items ) self . W = avg * np . random . rand ( n_users , n_factors ) # Unlike SGD, we explicitly set missing data to 0 so that it gets ignored in the multiplicative update. See Zhu, 2016 for a justification of using a binary mask matrix: https://arxiv.org/pdf/1612.06037.pdf self . dilate_mask ( n_samples = dilate_by_nsamples ) # fillna(0) is equivalent to hadamard (element-wise) product with a binary mask X = self . masked_data . fillna ( 0 ) . to_numpy () # Run multiplicative updating error_history , converged , n_iter , delta , norm_rmse , W , H = mult ( X , self . W , self . H , self . data_range , eps , tol , n_iterations , verbose , ) # Save outputs to model self . W , self . H = W , H self . error_history = error_history self . _n_iter = n_iter self . _delta = delta self . _norm_rmse = norm_rmse self . converged = converged if verbose : if self . converged : print ( \" \\n\\t CONVERGED!\" ) print ( f \" \\n\\t Final Iteration: { self . _n_iter } \" ) print ( f \" \\t Final Delta: { np . round ( self . _delta ) } \" ) else : print ( \" \\t FAILED TO CONVERGE (n_iter reached)\" ) print ( f \" \\n\\t Final Iteration: { self . _n_iter } \" ) print ( f \" \\t Final delta exceeds tol: { tol } <= { np . round ( self . _delta , 5 ) } \" ) print ( f \" \\t Final Norm Error: { np . round ( 100 * norm_rmse , 2 ) } %\" ) self . _predict () self . is_fit = True","title":"fit()"},{"location":"api/nmf_s/","text":"emotioncf.models.NNMF_sgd The non-negative matrix factorization algorithm tries to decompose a users x items matrix into two additional matrices: users x factors and factors x items. Training is performed via stochastic-gradient-descent and continues until convergence or the maximum number of iterations has been reached. Unlike NNMF_mult errors during training are used to update latent factors separately for each user/item combination. Additionally this implementation is more flexible as it supports hyperparameters for various kinds of regularization at the cost of increased computation time. The number of factors, convergence, and maximum iterations can be controlled with the n_factors , tol , and max_iterations arguments to the .fit method. By default the number of factors = the number items. fit ( self , n_factors = None , item_fact_reg = 0.0 , user_fact_reg = 0.0 , item_bias_reg = 0.0 , user_bias_reg = 0.0 , learning_rate = 0.001 , n_iterations = 1000 , tol = 1e-06 , verbose = False , dilate_by_nsamples = None , ** kwargs ) Fit NNMF collaborative filtering model using stochastic-gradient-descent Parameters: Name Type Description Default n_factors int number of factors to learn. Defaults to None which includes all factors. None item_fact_reg float item factor regularization to apply. Defaults to 0.0. 0.0 user_fact_reg float user factor regularization to apply. Defaults to 0.0. 0.0 item_bias_reg float item factor bias term to apply. Defaults to 0.0. 0.0 user_bias_reg float user factor bias term to apply. Defaults to 0.0. 0.0 learning_rate float how quickly to integrate errors during training. Defaults to 0.001. 0.001 n_iterations int total number of training iterations if convergence is not achieved. Defaults to 5000. 1000 tol float Convergence criteria. Model is considered converged if the change in error during training < tol. Defaults to 0.001. 1e-06 verbose bool print information about training. Defaults to False. False dilate_ts_n_samples int How many items to dilate by prior to training. Defaults to None. required save_learning bool Save error for each training iteration for diagnostic purposes. Set this to False if memory is a limitation and the n_iterations is very large. Defaults to True. required fast_sdg bool; optional Use an JIT compiled SGD for faster fitting. Note that verbose outputs are not compatible with this option and error history is always saved; Default False required Source code in emotioncf/models.py def fit ( self , n_factors = None , item_fact_reg = 0.0 , user_fact_reg = 0.0 , item_bias_reg = 0.0 , user_bias_reg = 0.0 , learning_rate = 0.001 , n_iterations = 1000 , tol = 1e-6 , verbose = False , dilate_by_nsamples = None , ** kwargs , ): \"\"\" Fit NNMF collaborative filtering model using stochastic-gradient-descent Args: n_factors (int, optional): number of factors to learn. Defaults to None which includes all factors. item_fact_reg (float, optional): item factor regularization to apply. Defaults to 0.0. user_fact_reg (float, optional): user factor regularization to apply. Defaults to 0.0. item_bias_reg (float, optional): item factor bias term to apply. Defaults to 0.0. user_bias_reg (float, optional): user factor bias term to apply. Defaults to 0.0. learning_rate (float, optional): how quickly to integrate errors during training. Defaults to 0.001. n_iterations (int, optional): total number of training iterations if convergence is not achieved. Defaults to 5000. tol (float, optional): Convergence criteria. Model is considered converged if the change in error during training < tol. Defaults to 0.001. verbose (bool, optional): print information about training. Defaults to False. dilate_ts_n_samples (int, optional): How many items to dilate by prior to training. Defaults to None. save_learning (bool, optional): Save error for each training iteration for diagnostic purposes. Set this to False if memory is a limitation and the n_iterations is very large. Defaults to True. fast_sdg (bool; optional): Use an JIT compiled SGD for faster fitting. Note that verbose outputs are not compatible with this option and error history is always saved; Default False \"\"\" # Call parent fit which acts as a guard for non-masked data super () . fit () # initialize variables n_users , n_items = self . data . shape if ( isinstance ( n_factors , int ) and n_factors >= n_items ) or isinstance ( n_factors , np . floating ): raise TypeError ( \"n_factors must be an integer < number of items\" ) if n_factors is None : n_factors = n_items self . n_factors = n_factors self . item_fact_reg = item_fact_reg self . user_fact_reg = user_fact_reg self . item_bias_reg = item_bias_reg self . user_bias_reg = user_bias_reg self . error_history = [] # Perform dilation if requested self . dilate_mask ( n_samples = dilate_by_nsamples ) # Get indices of missing data to compute if self . is_mask_dilated : sample_row , sample_col = self . dilated_mask . values . nonzero () else : sample_row , sample_col = self . mask . values . nonzero () # Convert tuples cause numba complains sample_row , sample_col = np . array ( sample_row ), np . array ( sample_col ) # Initialize global, user, and item biases and latent vectors self . global_bias = self . masked_data . mean () . mean () self . user_bias = np . zeros ( n_users ) self . item_bias = np . zeros ( n_items ) # Like multiplicative updating orient these as user x factor, factor x item self . user_vecs = np . random . normal ( scale = 1.0 / n_factors , size = ( n_users , n_factors ) ) self . item_vecs = np . random . normal ( scale = 1.0 / n_factors , size = ( n_factors , n_items ) ) X = self . masked_data . to_numpy () # Run SGD ( error_history , converged , n_iter , delta , norm_rmse , user_bias , user_vecs , item_bias , item_vecs , ) = sgd ( X , self . global_bias , self . data_range , tol , self . user_bias , self . user_vecs , self . user_bias_reg , self . user_fact_reg , self . item_bias , self . item_vecs , self . item_bias_reg , self . item_fact_reg , n_iterations , sample_row , sample_col , learning_rate , verbose , ) # Save outputs to model ( self . error_history , self . user_bias , self . user_vecs , self . item_bias , self . item_vecs , ) = ( error_history , user_bias , user_vecs , item_bias , item_vecs , ) self . _n_iter = n_iter self . _delta = delta self . _norm_rmse = norm_rmse self . converged = converged if verbose : if self . converged : print ( \" \\n\\t CONVERGED!\" ) print ( f \" \\n\\t Final Iteration: { self . _n_iter } \" ) print ( f \" \\t Final Delta: { np . round ( self . _delta ) } \" ) else : print ( \" \\t FAILED TO CONVERGE (n_iter reached)\" ) print ( f \" \\n\\t Final Iteration: { self . _n_iter } \" ) print ( f \" \\t Final delta exceeds tol: { tol } <= { np . round ( self . _delta , 5 ) } \" ) print ( f \" \\t Final Norm Error: { np . round ( 100 * norm_rmse , 2 ) } %\" ) self . _predict () self . is_fit = True","title":"NNMF_sgd"},{"location":"api/nmf_s/#emotioncfmodelsnnmf_sgd","text":"The non-negative matrix factorization algorithm tries to decompose a users x items matrix into two additional matrices: users x factors and factors x items. Training is performed via stochastic-gradient-descent and continues until convergence or the maximum number of iterations has been reached. Unlike NNMF_mult errors during training are used to update latent factors separately for each user/item combination. Additionally this implementation is more flexible as it supports hyperparameters for various kinds of regularization at the cost of increased computation time. The number of factors, convergence, and maximum iterations can be controlled with the n_factors , tol , and max_iterations arguments to the .fit method. By default the number of factors = the number items.","title":"emotioncf.models.NNMF_sgd"},{"location":"api/nmf_s/#emotioncf.models.NNMF_sgd.fit","text":"Fit NNMF collaborative filtering model using stochastic-gradient-descent Parameters: Name Type Description Default n_factors int number of factors to learn. Defaults to None which includes all factors. None item_fact_reg float item factor regularization to apply. Defaults to 0.0. 0.0 user_fact_reg float user factor regularization to apply. Defaults to 0.0. 0.0 item_bias_reg float item factor bias term to apply. Defaults to 0.0. 0.0 user_bias_reg float user factor bias term to apply. Defaults to 0.0. 0.0 learning_rate float how quickly to integrate errors during training. Defaults to 0.001. 0.001 n_iterations int total number of training iterations if convergence is not achieved. Defaults to 5000. 1000 tol float Convergence criteria. Model is considered converged if the change in error during training < tol. Defaults to 0.001. 1e-06 verbose bool print information about training. Defaults to False. False dilate_ts_n_samples int How many items to dilate by prior to training. Defaults to None. required save_learning bool Save error for each training iteration for diagnostic purposes. Set this to False if memory is a limitation and the n_iterations is very large. Defaults to True. required fast_sdg bool; optional Use an JIT compiled SGD for faster fitting. Note that verbose outputs are not compatible with this option and error history is always saved; Default False required Source code in emotioncf/models.py def fit ( self , n_factors = None , item_fact_reg = 0.0 , user_fact_reg = 0.0 , item_bias_reg = 0.0 , user_bias_reg = 0.0 , learning_rate = 0.001 , n_iterations = 1000 , tol = 1e-6 , verbose = False , dilate_by_nsamples = None , ** kwargs , ): \"\"\" Fit NNMF collaborative filtering model using stochastic-gradient-descent Args: n_factors (int, optional): number of factors to learn. Defaults to None which includes all factors. item_fact_reg (float, optional): item factor regularization to apply. Defaults to 0.0. user_fact_reg (float, optional): user factor regularization to apply. Defaults to 0.0. item_bias_reg (float, optional): item factor bias term to apply. Defaults to 0.0. user_bias_reg (float, optional): user factor bias term to apply. Defaults to 0.0. learning_rate (float, optional): how quickly to integrate errors during training. Defaults to 0.001. n_iterations (int, optional): total number of training iterations if convergence is not achieved. Defaults to 5000. tol (float, optional): Convergence criteria. Model is considered converged if the change in error during training < tol. Defaults to 0.001. verbose (bool, optional): print information about training. Defaults to False. dilate_ts_n_samples (int, optional): How many items to dilate by prior to training. Defaults to None. save_learning (bool, optional): Save error for each training iteration for diagnostic purposes. Set this to False if memory is a limitation and the n_iterations is very large. Defaults to True. fast_sdg (bool; optional): Use an JIT compiled SGD for faster fitting. Note that verbose outputs are not compatible with this option and error history is always saved; Default False \"\"\" # Call parent fit which acts as a guard for non-masked data super () . fit () # initialize variables n_users , n_items = self . data . shape if ( isinstance ( n_factors , int ) and n_factors >= n_items ) or isinstance ( n_factors , np . floating ): raise TypeError ( \"n_factors must be an integer < number of items\" ) if n_factors is None : n_factors = n_items self . n_factors = n_factors self . item_fact_reg = item_fact_reg self . user_fact_reg = user_fact_reg self . item_bias_reg = item_bias_reg self . user_bias_reg = user_bias_reg self . error_history = [] # Perform dilation if requested self . dilate_mask ( n_samples = dilate_by_nsamples ) # Get indices of missing data to compute if self . is_mask_dilated : sample_row , sample_col = self . dilated_mask . values . nonzero () else : sample_row , sample_col = self . mask . values . nonzero () # Convert tuples cause numba complains sample_row , sample_col = np . array ( sample_row ), np . array ( sample_col ) # Initialize global, user, and item biases and latent vectors self . global_bias = self . masked_data . mean () . mean () self . user_bias = np . zeros ( n_users ) self . item_bias = np . zeros ( n_items ) # Like multiplicative updating orient these as user x factor, factor x item self . user_vecs = np . random . normal ( scale = 1.0 / n_factors , size = ( n_users , n_factors ) ) self . item_vecs = np . random . normal ( scale = 1.0 / n_factors , size = ( n_factors , n_items ) ) X = self . masked_data . to_numpy () # Run SGD ( error_history , converged , n_iter , delta , norm_rmse , user_bias , user_vecs , item_bias , item_vecs , ) = sgd ( X , self . global_bias , self . data_range , tol , self . user_bias , self . user_vecs , self . user_bias_reg , self . user_fact_reg , self . item_bias , self . item_vecs , self . item_bias_reg , self . item_fact_reg , n_iterations , sample_row , sample_col , learning_rate , verbose , ) # Save outputs to model ( self . error_history , self . user_bias , self . user_vecs , self . item_bias , self . item_vecs , ) = ( error_history , user_bias , user_vecs , item_bias , item_vecs , ) self . _n_iter = n_iter self . _delta = delta self . _norm_rmse = norm_rmse self . converged = converged if verbose : if self . converged : print ( \" \\n\\t CONVERGED!\" ) print ( f \" \\n\\t Final Iteration: { self . _n_iter } \" ) print ( f \" \\t Final Delta: { np . round ( self . _delta ) } \" ) else : print ( \" \\t FAILED TO CONVERGE (n_iter reached)\" ) print ( f \" \\n\\t Final Iteration: { self . _n_iter } \" ) print ( f \" \\t Final delta exceeds tol: { tol } <= { np . round ( self . _delta , 5 ) } \" ) print ( f \" \\t Final Norm Error: { np . round ( 100 * norm_rmse , 2 ) } %\" ) self . _predict () self . is_fit = True","title":"fit()"},{"location":"api/utils/","text":"emotioncf.utils Utility functions and helpers create_sub_by_item_matrix ( df , columns = None , force_float = True , errors = 'raise' ) Convert a pandas long data frame of a single rating into a subject by item matrix Parameters: Name Type Description Default df Dataframe input dataframe required columns list list of length 3 with dataframe columns to use for reshaping. The first value should reflect unique individual identifier (\"Subject\"), the second a unique item identifier (\"Item\", \"Timepoint\"), and the last the rating made by the individual on that item (\"Rating\"). Defaults to [\"Subject\", \"Item\", \"Rating\"] None force_float bool force the resulting output to be float data types with errors being set to NaN; Default True True errors string how to handle errors in pd.to_numeric; Default 'raise' 'raise' Returns: Type Description pd.DataFrame user x item rating Dataframe Source code in emotioncf/utils.py def create_sub_by_item_matrix ( df , columns = None , force_float = True , errors = \"raise\" ): \"\"\"Convert a pandas long data frame of a single rating into a subject by item matrix Args: df (Dataframe): input dataframe columns (list): list of length 3 with dataframe columns to use for reshaping. The first value should reflect unique individual identifier (\"Subject\"), the second a unique item identifier (\"Item\", \"Timepoint\"), and the last the rating made by the individual on that item (\"Rating\"). Defaults to [\"Subject\", \"Item\", \"Rating\"] force_float (bool): force the resulting output to be float data types with errors being set to NaN; Default True errors (string): how to handle errors in pd.to_numeric; Default 'raise' Return: pd.DataFrame: user x item rating Dataframe \"\"\" if not isinstance ( df , pd . DataFrame ): raise ValueError ( \"df must be pandas instance\" ) if columns is None : columns = [ \"Subject\" , \"Item\" , \"Rating\" ] if not all ([ x in df . columns for x in columns ]): raise ValueError ( f \"df is missing some or all of the following columns: { columns } \" ) ratings = df [ columns ] ratings = ratings . pivot ( index = columns [ 0 ], columns = columns [ 1 ], values = columns [ 2 ]) try : if force_float : ratings = ratings . apply ( pd . to_numeric , errors = errors ) except ValueError as e : print ( \"Auto-converting data to floats failed, probably because you have non-numeric data in some rows. You can set errors = 'coerce' to set these failures to NaN\" ) raise ( e ) return ratings create_train_test_mask ( data , n_mask_items = 0.1 ) Given a pandas dataframe create a boolean mask such that n_mask_items columns are False and the rest are True . Critically, each row is masked independently. This function does not alter the input dataframe. Parameters: Name Type Description Default data pd.DataFrame input dataframe required n_train_items float if an integer is passed its raw value is used. Otherwise if a float is passed its taken to be a (rounded) percentage of the total items. Defaults to 0.1 (10% of the columns of data). required Exceptions: Type Description TypeError [description] Returns: Type Description pd.DataFrame boolean dataframe of same shape as data Source code in emotioncf/utils.py def create_train_test_mask ( data , n_mask_items = 0.1 ): \"\"\" Given a pandas dataframe create a boolean mask such that n_mask_items columns are `False` and the rest are `True`. Critically, each row is masked independently. This function does not alter the input dataframe. Args: data (pd.DataFrame): input dataframe n_train_items (float, optional): if an integer is passed its raw value is used. Otherwise if a float is passed its taken to be a (rounded) percentage of the total items. Defaults to 0.1 (10% of the columns of data). Raises: TypeError: [description] Returns: pd.DataFrame: boolean dataframe of same shape as data \"\"\" if data . isnull () . any () . any (): raise ValueError ( \"data already contains NaNs and further masking is ambiguous!\" ) if isinstance ( n_mask_items , ( float , np . floating )) and 1 >= n_mask_items > 0 : n_false_items = int ( np . round ( data . shape [ 1 ] * n_mask_items )) elif isinstance ( n_mask_items , ( int , np . integer )): n_false_items = n_mask_items else : raise TypeError ( f \"n_train_items must be an integer or a float between 0-1, not { type ( n_mask_items ) } with value { n_mask_items } \" ) n_true_items = data . shape [ 1 ] - n_false_items mask = np . array ([ True ] * n_true_items + [ False ] * n_false_items ) mask = np . vstack ( [ np . random . choice ( mask , replace = False , size = mask . shape ) for _ in range ( data . shape [ 0 ]) ] ) return pd . DataFrame ( mask , index = data . index , columns = data . columns ) downsample_dataframe ( data , n_samples , sampling_freq = None , target_type = 'samples' ) Down sample a dataframe Parameters: Name Type Description Default data pd.DataFrame input data required n_samples int number of samples. required sampling_freq int/float sampling frequency of data in hz. Defaults to None. None target_type str how to downsample; must be one of \"samples\", \"seconds\" or \"hz\". Defaults to \"samples\". 'samples' Returns: Type Description pd.DataFrame downsampled input data Source code in emotioncf/utils.py def downsample_dataframe ( data , n_samples , sampling_freq = None , target_type = \"samples\" ): \"\"\" Down sample a dataframe Args: data (pd.DataFrame): input data n_samples (int): number of samples. sampling_freq (int/float, optional): sampling frequency of data in hz. Defaults to None. target_type (str, optional): how to downsample; must be one of \"samples\", \"seconds\" or \"hz\". Defaults to \"samples\". Returns: pd.DataFrame: downsampled input data \"\"\" if not isinstance ( data , pd . DataFrame ): raise TypeError ( \"data must be a pandas dataframe\" ) if not isinstance ( n_samples , int ): raise TypeError ( \"n_samples must be an integer\" ) if target_type not in [ \"samples\" , \"seconds\" , \"hz\" ]: raise ValueError ( \"target_type must be 'samples', 'seconds', or 'hz'\" ) if target_type in [ \"seconds\" , \"hz\" ] and ( n_samples is None or sampling_freq is None ): raise ValueError ( f \"if target_type = { target_type } , both sampling_freq and target must be provided\" ) if target_type == \"seconds\" : n_samples = n_samples * sampling_freq elif target_type == \"hz\" : n_samples = sampling_freq / n_samples else : n_samples = n_samples data = data . T idx = np . sort ( np . repeat ( np . arange ( 1 , data . shape [ 0 ] / n_samples , 1 ), n_samples )) if data . shape [ 0 ] > len ( idx ): idx = np . concatenate ([ idx , np . repeat ( idx [ - 1 ] + 1 , data . shape [ 0 ] - len ( idx ))]) return data . groupby ( idx ) . mean () . T get_size_in_mb ( arr ) Calculates size of ndarray in megabytes Source code in emotioncf/utils.py def get_size_in_mb ( arr ): \"\"\"Calculates size of ndarray in megabytes\"\"\" if isinstance ( arr , ( np . ndarray , csr_matrix )): return arr . data . nbytes / 1e6 else : raise TypeError ( \"input must by a numpy array or scipy csr sparse matrix\" ) get_sparsity ( arr ) Calculates sparsity of ndarray (0 - 1) Source code in emotioncf/utils.py def get_sparsity ( arr ): \"\"\"Calculates sparsity of ndarray (0 - 1)\"\"\" if isinstance ( arr , ( np . ndarray )): return 1 - ( np . count_nonzero ( arr ) / arr . size ) else : raise TypeError ( \"input must be a numpy array\" ) load_movielens () Download and create a dataframe from the 100k movielens dataset Source code in emotioncf/utils.py def load_movielens (): # pragma: no cover \"\"\"Download and create a dataframe from the 100k movielens dataset\"\"\" url = \"http://files.grouplens.org/datasets/movielens/ml-100k.zip\" # With python context managers we don't need to save any temporary files print ( \"Getting movielens...\" ) try : with urlopen ( url ) as resp : with ZipFile ( BytesIO ( resp . read ())) as myzip : with myzip . open ( \"ml-100k/u.data\" ) as myfile : df = pd . read_csv ( myfile , delimiter = \" \\t \" , names = [ \"Subject\" , \"Item\" , \"Rating\" , \"Timestamp\" ], ) return df except Exception as e : print ( str ( e )) nanpdist ( arr , metric = 'euclidean' , return_square = True ) Just like scipy.spatial.distance.pdist or sklearn.metrics.pairwise_distances, but respects NaNs by only comparing the overlapping values from pairs of rows. Parameters: Name Type Description Default arr np.ndarray 2d array required metric str; optional distance metric to use. Must be supported by scipy 'euclidean' return_square boo; optional return a symmetric 2d distance matrix like sklearn instead of a 1d vector like pdist; Default True True Returns: Type Description np.ndarray symmetric 2d array of distances Source code in emotioncf/utils.py def nanpdist ( arr , metric = \"euclidean\" , return_square = True ): \"\"\" Just like scipy.spatial.distance.pdist or sklearn.metrics.pairwise_distances, but respects NaNs by only comparing the overlapping values from pairs of rows. Args: arr (np.ndarray): 2d array metric (str; optional): distance metric to use. Must be supported by scipy return_square (boo; optional): return a symmetric 2d distance matrix like sklearn instead of a 1d vector like pdist; Default True Return: np.ndarray: symmetric 2d array of distances \"\"\" has_nans = pd . DataFrame ( arr ) . isnull () . any () . any () if not has_nans : out = pdist ( arr , metric = metric ) else : nrows = arr . shape [ 0 ] out = np . zeros ( nrows * ( nrows - 1 ) // 2 , dtype = float ) mask = np . isfinite ( arr ) vec_mask = np . zeros ( arr . shape [ 1 ], dtype = bool ) k = 0 for row1_idx in range ( nrows - 1 ): for row2_idx in range ( row1_idx + 1 , nrows ): vec_mask = np . logical_and ( mask [ row1_idx ], mask [ row2_idx ]) masked_row1 , masked_row2 = ( arr [ row1_idx ][ vec_mask ], arr [ row2_idx ][ vec_mask ], ) out [ k ] = pdist ( np . vstack ([ masked_row1 , masked_row2 ]), metric = metric ) k += 1 if return_square : if out . ndim == 1 : out = squareform ( out ) else : if out . ndim == 2 : out = squareform ( out ) return out","title":"emotioncf.utils"},{"location":"api/utils/#emotioncfutils","text":"Utility functions and helpers","title":"emotioncf.utils"},{"location":"api/utils/#emotioncf.utils.create_sub_by_item_matrix","text":"Convert a pandas long data frame of a single rating into a subject by item matrix Parameters: Name Type Description Default df Dataframe input dataframe required columns list list of length 3 with dataframe columns to use for reshaping. The first value should reflect unique individual identifier (\"Subject\"), the second a unique item identifier (\"Item\", \"Timepoint\"), and the last the rating made by the individual on that item (\"Rating\"). Defaults to [\"Subject\", \"Item\", \"Rating\"] None force_float bool force the resulting output to be float data types with errors being set to NaN; Default True True errors string how to handle errors in pd.to_numeric; Default 'raise' 'raise' Returns: Type Description pd.DataFrame user x item rating Dataframe Source code in emotioncf/utils.py def create_sub_by_item_matrix ( df , columns = None , force_float = True , errors = \"raise\" ): \"\"\"Convert a pandas long data frame of a single rating into a subject by item matrix Args: df (Dataframe): input dataframe columns (list): list of length 3 with dataframe columns to use for reshaping. The first value should reflect unique individual identifier (\"Subject\"), the second a unique item identifier (\"Item\", \"Timepoint\"), and the last the rating made by the individual on that item (\"Rating\"). Defaults to [\"Subject\", \"Item\", \"Rating\"] force_float (bool): force the resulting output to be float data types with errors being set to NaN; Default True errors (string): how to handle errors in pd.to_numeric; Default 'raise' Return: pd.DataFrame: user x item rating Dataframe \"\"\" if not isinstance ( df , pd . DataFrame ): raise ValueError ( \"df must be pandas instance\" ) if columns is None : columns = [ \"Subject\" , \"Item\" , \"Rating\" ] if not all ([ x in df . columns for x in columns ]): raise ValueError ( f \"df is missing some or all of the following columns: { columns } \" ) ratings = df [ columns ] ratings = ratings . pivot ( index = columns [ 0 ], columns = columns [ 1 ], values = columns [ 2 ]) try : if force_float : ratings = ratings . apply ( pd . to_numeric , errors = errors ) except ValueError as e : print ( \"Auto-converting data to floats failed, probably because you have non-numeric data in some rows. You can set errors = 'coerce' to set these failures to NaN\" ) raise ( e ) return ratings","title":"create_sub_by_item_matrix()"},{"location":"api/utils/#emotioncf.utils.create_train_test_mask","text":"Given a pandas dataframe create a boolean mask such that n_mask_items columns are False and the rest are True . Critically, each row is masked independently. This function does not alter the input dataframe. Parameters: Name Type Description Default data pd.DataFrame input dataframe required n_train_items float if an integer is passed its raw value is used. Otherwise if a float is passed its taken to be a (rounded) percentage of the total items. Defaults to 0.1 (10% of the columns of data). required Exceptions: Type Description TypeError [description] Returns: Type Description pd.DataFrame boolean dataframe of same shape as data Source code in emotioncf/utils.py def create_train_test_mask ( data , n_mask_items = 0.1 ): \"\"\" Given a pandas dataframe create a boolean mask such that n_mask_items columns are `False` and the rest are `True`. Critically, each row is masked independently. This function does not alter the input dataframe. Args: data (pd.DataFrame): input dataframe n_train_items (float, optional): if an integer is passed its raw value is used. Otherwise if a float is passed its taken to be a (rounded) percentage of the total items. Defaults to 0.1 (10% of the columns of data). Raises: TypeError: [description] Returns: pd.DataFrame: boolean dataframe of same shape as data \"\"\" if data . isnull () . any () . any (): raise ValueError ( \"data already contains NaNs and further masking is ambiguous!\" ) if isinstance ( n_mask_items , ( float , np . floating )) and 1 >= n_mask_items > 0 : n_false_items = int ( np . round ( data . shape [ 1 ] * n_mask_items )) elif isinstance ( n_mask_items , ( int , np . integer )): n_false_items = n_mask_items else : raise TypeError ( f \"n_train_items must be an integer or a float between 0-1, not { type ( n_mask_items ) } with value { n_mask_items } \" ) n_true_items = data . shape [ 1 ] - n_false_items mask = np . array ([ True ] * n_true_items + [ False ] * n_false_items ) mask = np . vstack ( [ np . random . choice ( mask , replace = False , size = mask . shape ) for _ in range ( data . shape [ 0 ]) ] ) return pd . DataFrame ( mask , index = data . index , columns = data . columns )","title":"create_train_test_mask()"},{"location":"api/utils/#emotioncf.utils.downsample_dataframe","text":"Down sample a dataframe Parameters: Name Type Description Default data pd.DataFrame input data required n_samples int number of samples. required sampling_freq int/float sampling frequency of data in hz. Defaults to None. None target_type str how to downsample; must be one of \"samples\", \"seconds\" or \"hz\". Defaults to \"samples\". 'samples' Returns: Type Description pd.DataFrame downsampled input data Source code in emotioncf/utils.py def downsample_dataframe ( data , n_samples , sampling_freq = None , target_type = \"samples\" ): \"\"\" Down sample a dataframe Args: data (pd.DataFrame): input data n_samples (int): number of samples. sampling_freq (int/float, optional): sampling frequency of data in hz. Defaults to None. target_type (str, optional): how to downsample; must be one of \"samples\", \"seconds\" or \"hz\". Defaults to \"samples\". Returns: pd.DataFrame: downsampled input data \"\"\" if not isinstance ( data , pd . DataFrame ): raise TypeError ( \"data must be a pandas dataframe\" ) if not isinstance ( n_samples , int ): raise TypeError ( \"n_samples must be an integer\" ) if target_type not in [ \"samples\" , \"seconds\" , \"hz\" ]: raise ValueError ( \"target_type must be 'samples', 'seconds', or 'hz'\" ) if target_type in [ \"seconds\" , \"hz\" ] and ( n_samples is None or sampling_freq is None ): raise ValueError ( f \"if target_type = { target_type } , both sampling_freq and target must be provided\" ) if target_type == \"seconds\" : n_samples = n_samples * sampling_freq elif target_type == \"hz\" : n_samples = sampling_freq / n_samples else : n_samples = n_samples data = data . T idx = np . sort ( np . repeat ( np . arange ( 1 , data . shape [ 0 ] / n_samples , 1 ), n_samples )) if data . shape [ 0 ] > len ( idx ): idx = np . concatenate ([ idx , np . repeat ( idx [ - 1 ] + 1 , data . shape [ 0 ] - len ( idx ))]) return data . groupby ( idx ) . mean () . T","title":"downsample_dataframe()"},{"location":"api/utils/#emotioncf.utils.get_size_in_mb","text":"Calculates size of ndarray in megabytes Source code in emotioncf/utils.py def get_size_in_mb ( arr ): \"\"\"Calculates size of ndarray in megabytes\"\"\" if isinstance ( arr , ( np . ndarray , csr_matrix )): return arr . data . nbytes / 1e6 else : raise TypeError ( \"input must by a numpy array or scipy csr sparse matrix\" )","title":"get_size_in_mb()"},{"location":"api/utils/#emotioncf.utils.get_sparsity","text":"Calculates sparsity of ndarray (0 - 1) Source code in emotioncf/utils.py def get_sparsity ( arr ): \"\"\"Calculates sparsity of ndarray (0 - 1)\"\"\" if isinstance ( arr , ( np . ndarray )): return 1 - ( np . count_nonzero ( arr ) / arr . size ) else : raise TypeError ( \"input must be a numpy array\" )","title":"get_sparsity()"},{"location":"api/utils/#emotioncf.utils.load_movielens","text":"Download and create a dataframe from the 100k movielens dataset Source code in emotioncf/utils.py def load_movielens (): # pragma: no cover \"\"\"Download and create a dataframe from the 100k movielens dataset\"\"\" url = \"http://files.grouplens.org/datasets/movielens/ml-100k.zip\" # With python context managers we don't need to save any temporary files print ( \"Getting movielens...\" ) try : with urlopen ( url ) as resp : with ZipFile ( BytesIO ( resp . read ())) as myzip : with myzip . open ( \"ml-100k/u.data\" ) as myfile : df = pd . read_csv ( myfile , delimiter = \" \\t \" , names = [ \"Subject\" , \"Item\" , \"Rating\" , \"Timestamp\" ], ) return df except Exception as e : print ( str ( e ))","title":"load_movielens()"},{"location":"api/utils/#emotioncf.utils.nanpdist","text":"Just like scipy.spatial.distance.pdist or sklearn.metrics.pairwise_distances, but respects NaNs by only comparing the overlapping values from pairs of rows. Parameters: Name Type Description Default arr np.ndarray 2d array required metric str; optional distance metric to use. Must be supported by scipy 'euclidean' return_square boo; optional return a symmetric 2d distance matrix like sklearn instead of a 1d vector like pdist; Default True True Returns: Type Description np.ndarray symmetric 2d array of distances Source code in emotioncf/utils.py def nanpdist ( arr , metric = \"euclidean\" , return_square = True ): \"\"\" Just like scipy.spatial.distance.pdist or sklearn.metrics.pairwise_distances, but respects NaNs by only comparing the overlapping values from pairs of rows. Args: arr (np.ndarray): 2d array metric (str; optional): distance metric to use. Must be supported by scipy return_square (boo; optional): return a symmetric 2d distance matrix like sklearn instead of a 1d vector like pdist; Default True Return: np.ndarray: symmetric 2d array of distances \"\"\" has_nans = pd . DataFrame ( arr ) . isnull () . any () . any () if not has_nans : out = pdist ( arr , metric = metric ) else : nrows = arr . shape [ 0 ] out = np . zeros ( nrows * ( nrows - 1 ) // 2 , dtype = float ) mask = np . isfinite ( arr ) vec_mask = np . zeros ( arr . shape [ 1 ], dtype = bool ) k = 0 for row1_idx in range ( nrows - 1 ): for row2_idx in range ( row1_idx + 1 , nrows ): vec_mask = np . logical_and ( mask [ row1_idx ], mask [ row2_idx ]) masked_row1 , masked_row2 = ( arr [ row1_idx ][ vec_mask ], arr [ row2_idx ][ vec_mask ], ) out [ k ] = pdist ( np . vstack ([ masked_row1 , masked_row2 ]), metric = metric ) k += 1 if return_square : if out . ndim == 1 : out = squareform ( out ) else : if out . ndim == 2 : out = squareform ( out ) return out","title":"nanpdist()"}]}