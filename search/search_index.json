{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Emotion CF A Python package for collaborative filtering on emotion datasets Installation pip install git+https://github.com/cosanlab/emotionCF.git Getting started Checkout the quick overview for examples to help you get started. Or check out the API reference on the left to explore the details of specific models. A unique feature of this toolbox is its support for working with time-series data . Algorithms Currently supported algorithms include: Mean - a baseline model KNN - k-nearest neighbors NNMF_mult - non-negative matrix factorization trained via multiplicative updating NNMF_sgd - non-negative matrix factorization trained via stochastic gradient descent","title":"Home"},{"location":"#emotion-cf","text":"","title":"Emotion CF"},{"location":"#a-python-package-for-collaborative-filtering-on-emotion-datasets","text":"","title":"A Python package for collaborative filtering on emotion datasets"},{"location":"#installation","text":"pip install git+https://github.com/cosanlab/emotionCF.git","title":"Installation"},{"location":"#getting-started","text":"Checkout the quick overview for examples to help you get started. Or check out the API reference on the left to explore the details of specific models. A unique feature of this toolbox is its support for working with time-series data .","title":"Getting started"},{"location":"#algorithms","text":"Currently supported algorithms include: Mean - a baseline model KNN - k-nearest neighbors NNMF_mult - non-negative matrix factorization trained via multiplicative updating NNMF_sgd - non-negative matrix factorization trained via stochastic gradient descent","title":"Algorithms"},{"location":"development/","text":"Development Each new push or pull-request to the code base for this toolbox will automatically be run through testing and documentation building via github actions. To develop this package or its documentation locally you will need to install a few extra dependencies. Installation pip install -r requirements-dev.txt Testing To run tests just call pytest from the root of this repository. New tests can be added in emotioncf/tests/ . Formatting Please format your code using black. If you've installed the development dependencies, then you can configure git to tell if you any new changes are not formatted by setting up a pre-commit hook: cd .git/hooks Create a new file called pre-commit with the following contents: #!/bin/sh black --check . - Make sure the file is executable chmod 775 pre-commit Now anytime you try to commit new changes, git will automatically run black before the commit and warn you if certain files need to be formatted. Editing continuous integration To change how the automatic workflow builds are specified, make the relevant edits in .github/workflows/conda_ci.yml . Documentation Documentation is built with mkdocs using the mkdocs material theme and mkdocstrings extension. Live server After installation above, simply run mkdocs serve this the project root to start a hot-reloading server of the documentation at http://localhost:8000 . To alter the layout of the docs site adjust settings in mkdocs.yml . To add or edit pages simply create markdown files within the docs/ folder. Deploying You can use the mkdocs gh-deploy command in order to build and push the documentation site to the github-pages branch of this repo.","title":"Development"},{"location":"development/#development","text":"Each new push or pull-request to the code base for this toolbox will automatically be run through testing and documentation building via github actions. To develop this package or its documentation locally you will need to install a few extra dependencies.","title":"Development"},{"location":"development/#installation","text":"pip install -r requirements-dev.txt","title":"Installation"},{"location":"development/#testing","text":"To run tests just call pytest from the root of this repository. New tests can be added in emotioncf/tests/ .","title":"Testing"},{"location":"development/#formatting","text":"Please format your code using black. If you've installed the development dependencies, then you can configure git to tell if you any new changes are not formatted by setting up a pre-commit hook: cd .git/hooks Create a new file called pre-commit with the following contents: #!/bin/sh black --check . - Make sure the file is executable chmod 775 pre-commit Now anytime you try to commit new changes, git will automatically run black before the commit and warn you if certain files need to be formatted.","title":"Formatting"},{"location":"development/#editing-continuous-integration","text":"To change how the automatic workflow builds are specified, make the relevant edits in .github/workflows/conda_ci.yml .","title":"Editing continuous integration"},{"location":"development/#documentation","text":"Documentation is built with mkdocs using the mkdocs material theme and mkdocstrings extension.","title":"Documentation"},{"location":"development/#live-server","text":"After installation above, simply run mkdocs serve this the project root to start a hot-reloading server of the documentation at http://localhost:8000 . To alter the layout of the docs site adjust settings in mkdocs.yml . To add or edit pages simply create markdown files within the docs/ folder.","title":"Live server"},{"location":"development/#deploying","text":"You can use the mkdocs gh-deploy command in order to build and push the documentation site to the github-pages branch of this repo.","title":"Deploying"},{"location":"overview/","text":"Quick Overview Example Usage All algorithms in the emotionCF toolbox operates on 2d pandas dataframes with rows as unique users and columns as unique items . Create a subject by item matrix The toolbox contains a helper function create_sub_by_item_matrix() to convert a long-form dataframe into this format provided it has 3 columns named ['Subject', 'Item', 'Rating'] or equivalent (see the columns keyword argument of create_sub_by_item_matrix ). from emotioncf import create_sub_by_item_matrix ratings = create_sub_by_item_matrix ( long_format_df ) Initialize a model instance Before you can fit a model on your data you need to initialize a new model instance by passing in your user x item dataframe: from emotioncf import KNN cf = KNN ( ratings ) Split Data into Train and Test It is easy to split your data into training and test sets using the split_train_test() method on any model instance. This creates a binary mask saved into the model object itself and accessible at model.train_mask . Unlike libraries like sklearn , splitting data this way chooses a different random subset of items per user to perform training. All users are always included in training set, but each user will have a different random set of items . # use 50% of the items for training per user cf . split_train_test ( n_train_items =. 5 ) # contains the training mask: cf . train_mask Estimate Model Each model can be estimated using its fit() method. If no data splitting is done, then all data is used for training. cf . fit () Predict New Ratings To generate predictions, used a model's predict() method. This creates a pandas dataframe of the predicted subject by item matrix within the model object itself, accessible at model.predicted_ratings . cf . predict () Evaluate Model Predictions There are several methods to aid in evaluating the performance of the model including: - overall mean squared error .get_mse() - overall correlation .get_corr() - mean-squared-error separately per user .get_sub_mse() - correlation for each subject .get_sub_corr() Additionally, each method takes a string argument to indicate what dataset performance should be calculated for: 'train' , 'test' , or 'all' . Defaults to 'test' . cf . get_mse ( 'all' ) cf . get_corr () # 'test' cf . get_sub_corr ( 'train' ) Working with Time-Series Data A unique feature of this toolbox, is that it has also been designed to work with time-series data. For example, model instances can be downsampled across items, where items refers to time samples. You must specify the sampling_freq of the data, and the target , where target must have a target_type of ['hz','samples','seconds'] . Downsampling is performed by averaging over bin windows. In this example we downsample a dataset from 10Hz to 5Hz. cf . downsample ( sampling_freq = 10 , target = 5 , target_type = 'hz' ) It is also possible to leverage presumed autocorrelation when training models by using the dilate_ts_n_samples=n_samples keyword. This flag will convolve a boxcar kernel of width n_samples with each user's rating from model.train_mask . The dilation will be centered on each sample. The intuition here is that if a subject rates an item at a given time point, say '50', they likely will have rated time points immediately preceding and following similarly (e.g., [50,50,50] ). This is due to autocorrelation in the data. More presumed autocorrelation will likely benefit from a higher number of samples being selected. This will allow time series that are sparsely sampled to be estimated more accurately. cf = NNMF_sgd ( ratings ) mask = cf . train_mask cf . split_train_test ( n_train_items = 20 ) cf . fit ( n_iterations = 100 , user_fact_reg = 1.0 , item_fact_reg = 0.001 , user_bias_reg = 0 , item_bias_reg = 0 , learning_rate =. 001 , dilate_ts_n_samples = 20 ) cf . predict () cf . get_mse ( 'test' ) cf . get_corr ( 'test' ) cf . plot_predictions () Supported Algorithms Mean An easy control model for collaborative filtering is to demonstrate how well the models perform over simply using the item means. We initialize a class instance and then the model can be estimated and new ratings predicted. We can get the overall mean squared error on the predicted ratings. from emotioncf.cf import Mean cf = Mean ( ratings ) cf . fit () cf . predict () cf . get_mse ( 'all' ) K-Nearest Neighbors EmotionCF uses a standard API to estimate and predict data. Though the KNN approach is not technically a model, we still use the fit method to estimate data. This calculates a similarity matrix between subjects using ['correlation','cosine'] methods. We can then predict the left out ratings using the top k nearest neighbors. We can evaluate how well the model works for all data points using get_corr() and get_mse() methods. We can also get the correlation for each subject's individual data using get_sub_corr() method. So far we have found that this method does not perform well when there aren't many overlapping samples across items and users. from emotioncf import KNN cf = KNN ( ratings ) cf . split_train_test ( n_train_items = 20 ) cf . fit ( metric = 'pearson' ) cf . predict ( k = 10 ) cf . get_mse ( 'test' ) cf . get_corr ( 'test' ) cf . get_sub_corr ( 'test' ) Non-negative matrix factorization using stochastic gradient descent Here we initialize a new class instance and split the data into 20 training and 80 test items per subject. We fit the model using 100 iterations. Can pass in optional regularization parameters and a learning rate for the update function. The model is then used to predict the left out ratings. We can get the overall model MSE and correlation value on the test ratings. We can also make a quick plot of the results. As indicated by the name, this method does not work with data that includes negative numbers. from emotioncf import NNMF_sgd cf = NNMF_sgd ( ratings ) cf . split_train_test ( n_train_items = 20 ) cf . fit ( n_iterations = 100 , user_fact_reg = 0 , item_fact_reg = 0 , user_bias_reg = 0 , item_bias_reg = 0 , learning_rate =. 001 ) cf . predict () cf . get_mse ( 'test' ) cf . get_corr ( 'test' ) cf . plot_predictions () Non-negative matrix factorization using multiplicative updating Similarly, we can fit a different NNMF model that uses multiplicative updating with the NNMF_mult class. from emotioncf import NNMF_mult cf = NNMF_mult ( ratings ) cf . split_train_test ( n_train_items = 20 ) cf . fit ( max_iterations = 200 ) cf . predict () cf . get_mse ( 'test' ) cf . get_corr ( 'test' ) cf . plot_predictions ()","title":"Quick Overview"},{"location":"overview/#quick-overview","text":"","title":"Quick Overview"},{"location":"overview/#example-usage","text":"All algorithms in the emotionCF toolbox operates on 2d pandas dataframes with rows as unique users and columns as unique items .","title":"Example Usage"},{"location":"overview/#create-a-subject-by-item-matrix","text":"The toolbox contains a helper function create_sub_by_item_matrix() to convert a long-form dataframe into this format provided it has 3 columns named ['Subject', 'Item', 'Rating'] or equivalent (see the columns keyword argument of create_sub_by_item_matrix ). from emotioncf import create_sub_by_item_matrix ratings = create_sub_by_item_matrix ( long_format_df )","title":"Create a subject by item matrix"},{"location":"overview/#initialize-a-model-instance","text":"Before you can fit a model on your data you need to initialize a new model instance by passing in your user x item dataframe: from emotioncf import KNN cf = KNN ( ratings )","title":"Initialize a model instance"},{"location":"overview/#split-data-into-train-and-test","text":"It is easy to split your data into training and test sets using the split_train_test() method on any model instance. This creates a binary mask saved into the model object itself and accessible at model.train_mask . Unlike libraries like sklearn , splitting data this way chooses a different random subset of items per user to perform training. All users are always included in training set, but each user will have a different random set of items . # use 50% of the items for training per user cf . split_train_test ( n_train_items =. 5 ) # contains the training mask: cf . train_mask","title":"Split Data into Train and Test"},{"location":"overview/#estimate-model","text":"Each model can be estimated using its fit() method. If no data splitting is done, then all data is used for training. cf . fit ()","title":"Estimate Model"},{"location":"overview/#predict-new-ratings","text":"To generate predictions, used a model's predict() method. This creates a pandas dataframe of the predicted subject by item matrix within the model object itself, accessible at model.predicted_ratings . cf . predict ()","title":"Predict New Ratings"},{"location":"overview/#evaluate-model-predictions","text":"There are several methods to aid in evaluating the performance of the model including: - overall mean squared error .get_mse() - overall correlation .get_corr() - mean-squared-error separately per user .get_sub_mse() - correlation for each subject .get_sub_corr() Additionally, each method takes a string argument to indicate what dataset performance should be calculated for: 'train' , 'test' , or 'all' . Defaults to 'test' . cf . get_mse ( 'all' ) cf . get_corr () # 'test' cf . get_sub_corr ( 'train' )","title":"Evaluate Model Predictions"},{"location":"overview/#working-with-time-series-data","text":"A unique feature of this toolbox, is that it has also been designed to work with time-series data. For example, model instances can be downsampled across items, where items refers to time samples. You must specify the sampling_freq of the data, and the target , where target must have a target_type of ['hz','samples','seconds'] . Downsampling is performed by averaging over bin windows. In this example we downsample a dataset from 10Hz to 5Hz. cf . downsample ( sampling_freq = 10 , target = 5 , target_type = 'hz' ) It is also possible to leverage presumed autocorrelation when training models by using the dilate_ts_n_samples=n_samples keyword. This flag will convolve a boxcar kernel of width n_samples with each user's rating from model.train_mask . The dilation will be centered on each sample. The intuition here is that if a subject rates an item at a given time point, say '50', they likely will have rated time points immediately preceding and following similarly (e.g., [50,50,50] ). This is due to autocorrelation in the data. More presumed autocorrelation will likely benefit from a higher number of samples being selected. This will allow time series that are sparsely sampled to be estimated more accurately. cf = NNMF_sgd ( ratings ) mask = cf . train_mask cf . split_train_test ( n_train_items = 20 ) cf . fit ( n_iterations = 100 , user_fact_reg = 1.0 , item_fact_reg = 0.001 , user_bias_reg = 0 , item_bias_reg = 0 , learning_rate =. 001 , dilate_ts_n_samples = 20 ) cf . predict () cf . get_mse ( 'test' ) cf . get_corr ( 'test' ) cf . plot_predictions ()","title":"Working with Time-Series Data"},{"location":"overview/#supported-algorithms","text":"","title":"Supported Algorithms"},{"location":"overview/#mean","text":"An easy control model for collaborative filtering is to demonstrate how well the models perform over simply using the item means. We initialize a class instance and then the model can be estimated and new ratings predicted. We can get the overall mean squared error on the predicted ratings. from emotioncf.cf import Mean cf = Mean ( ratings ) cf . fit () cf . predict () cf . get_mse ( 'all' )","title":"Mean"},{"location":"overview/#k-nearest-neighbors","text":"EmotionCF uses a standard API to estimate and predict data. Though the KNN approach is not technically a model, we still use the fit method to estimate data. This calculates a similarity matrix between subjects using ['correlation','cosine'] methods. We can then predict the left out ratings using the top k nearest neighbors. We can evaluate how well the model works for all data points using get_corr() and get_mse() methods. We can also get the correlation for each subject's individual data using get_sub_corr() method. So far we have found that this method does not perform well when there aren't many overlapping samples across items and users. from emotioncf import KNN cf = KNN ( ratings ) cf . split_train_test ( n_train_items = 20 ) cf . fit ( metric = 'pearson' ) cf . predict ( k = 10 ) cf . get_mse ( 'test' ) cf . get_corr ( 'test' ) cf . get_sub_corr ( 'test' )","title":"K-Nearest Neighbors"},{"location":"overview/#non-negative-matrix-factorization-using-stochastic-gradient-descent","text":"Here we initialize a new class instance and split the data into 20 training and 80 test items per subject. We fit the model using 100 iterations. Can pass in optional regularization parameters and a learning rate for the update function. The model is then used to predict the left out ratings. We can get the overall model MSE and correlation value on the test ratings. We can also make a quick plot of the results. As indicated by the name, this method does not work with data that includes negative numbers. from emotioncf import NNMF_sgd cf = NNMF_sgd ( ratings ) cf . split_train_test ( n_train_items = 20 ) cf . fit ( n_iterations = 100 , user_fact_reg = 0 , item_fact_reg = 0 , user_bias_reg = 0 , item_bias_reg = 0 , learning_rate =. 001 ) cf . predict () cf . get_mse ( 'test' ) cf . get_corr ( 'test' ) cf . plot_predictions ()","title":"Non-negative matrix factorization using stochastic gradient descent"},{"location":"overview/#non-negative-matrix-factorization-using-multiplicative-updating","text":"Similarly, we can fit a different NNMF model that uses multiplicative updating with the NNMF_mult class. from emotioncf import NNMF_mult cf = NNMF_mult ( ratings ) cf . split_train_test ( n_train_items = 20 ) cf . fit ( max_iterations = 200 ) cf . predict () cf . get_mse ( 'test' ) cf . get_corr ( 'test' ) cf . plot_predictions ()","title":"Non-negative matrix factorization using multiplicative updating"},{"location":"api/base/","text":"emotioncf.base Base algorithm classes. All other algorithms inherit from these classes which means they have access to all their methods and attributes. You won't typically utilize these classes directly unless you're creating a custom estimator. Base All other models and base classes inherit from this class. downsample ( self , sampling_freq = None , target = None , target_type = 'samples' ) Downsample rating matrix to a new target frequency or number of samples using averaging. Parameters: Name Type Description Default sampling_freq int/float Sampling frequency of data None target int/float downsampling target None target_type str type of target can be [samples,seconds,hz] 'samples' Source code in emotioncf/base.py def downsample ( self , sampling_freq = None , target = None , target_type = \"samples\" ): \"\"\"Downsample rating matrix to a new target frequency or number of samples using averaging. Args: sampling_freq (int/float): Sampling frequency of data target (int/float): downsampling target target_type (str): type of target can be [samples,seconds,hz] \"\"\" if sampling_freq is None : raise ValueError ( \"Please specify the sampling frequency of the data.\" ) if target is None : raise ValueError ( \"Please specify the downsampling target.\" ) if target_type is None : raise ValueError ( \"Please specify the type of target to downsample to [samples,seconds,hz].\" ) def ds ( data , sampling_freq = sampling_freq , target = None , target_type = \"samples\" ): if target_type == \"samples\" : n_samples = target elif target_type == \"seconds\" : n_samples = target * sampling_freq elif target_type == \"hz\" : n_samples = sampling_freq / target else : raise ValueError ( 'Make sure target_type is \"samples\", \"seconds\", or \"hz\".' ) data = data . T idx = np . sort ( np . repeat ( np . arange ( 1 , data . shape [ 0 ] / n_samples , 1 ), n_samples ) ) if data . shape [ 0 ] > len ( idx ): idx = np . concatenate ( [ idx , np . repeat ( idx [ - 1 ] + 1 , data . shape [ 0 ] - len ( idx ))] ) return data . groupby ( idx ) . mean () . T self . data = ds ( self . data , sampling_freq = sampling_freq , target = target , target_type = target_type , ) if self . is_mask : self . train_mask = ds ( self . train_mask , sampling_freq = sampling_freq , target = target , target_type = target_type , ) self . train_mask . loc [:, :] = self . train_mask > 0 self . masked_data = ds ( self . masked_data , sampling_freq = sampling_freq , target = target , target_type = target_type , ) if self . is_mask_dilated : self . dilated_mask = ds ( self . dilated_mask , sampling_freq = sampling_freq , target = target , target_type = target_type , ) self . dilated_mask . loc [:, :] = self . dilated_mask > 0 if self . is_predict : self . predictions = ds ( self . predictions , sampling_freq = sampling_freq , target = target , target_type = target_type , ) get_corr ( self , dataset = 'test' ) Get overall correlation for predicted compared to actual for all items and subjects. Parameters: Name Type Description Default dataset str Get correlation on 'all' data, the 'train' data, or the 'test' data 'test' Returns: Type Description r (float) Correlation Source code in emotioncf/base.py def get_corr ( self , dataset = \"test\" ): \"\"\"Get overall correlation for predicted compared to actual for all items and subjects. Args: dataset (str): Get correlation on 'all' data, the 'train' data, or the 'test' data Returns: r (float): Correlation \"\"\" if not self . is_fit : raise ValueError ( \"You must fit() model first before using this method.\" ) if not self . is_predict : raise ValueError ( \"You must predict() model first before using this method.\" ) actual , pred = self . _retrieve_predictions ( dataset ) # Handle nans when computing correlation nans = np . logical_or ( np . isnan ( actual ), np . isnan ( pred )) return pearsonr ( actual [ ~ nans ], pred [ ~ nans ])[ 0 ] get_mse ( self , dataset = 'test' ) Get overall mean squared error for predicted compared to actual for all items and subjects. Parameters: Name Type Description Default dataset str Get mse on 'all' data, the 'train' data, or the 'test' data 'test' Returns: Type Description mse (float) mean squared error Source code in emotioncf/base.py def get_mse ( self , dataset = \"test\" ): \"\"\"Get overall mean squared error for predicted compared to actual for all items and subjects. Args: dataset (str): Get mse on 'all' data, the 'train' data, or the 'test' data Returns: mse (float): mean squared error \"\"\" if not self . is_fit : raise ValueError ( \"You must fit() model first before using this method.\" ) if not self . is_predict : raise ValueError ( \"You must predict() model first before using this method.\" ) actual , pred = self . _retrieve_predictions ( dataset ) return np . nanmean (( pred - actual ) ** 2 ) get_sub_corr ( self , dataset = 'test' ) Calculate observed/predicted correlation for each subject in matrix Parameters: Name Type Description Default dataset str Get correlation on 'all' data, the 'train' data, or the 'test' data 'test' Returns: Type Description r (float) Correlation Source code in emotioncf/base.py def get_sub_corr ( self , dataset = \"test\" ): \"\"\"Calculate observed/predicted correlation for each subject in matrix Args: dataset (str): Get correlation on 'all' data, the 'train' data, or the 'test' data Returns: r (float): Correlation \"\"\" if not self . is_fit : raise ValueError ( \"You must fit() model first before using this method.\" ) if not self . is_predict : raise ValueError ( \"You must predict() model first before using this method.\" ) r = [] # Note: the following mask prevents NaN values from being passed to `pearsonr()`. # However, it does not guaratee that no correlation values will be NaN, e.g. if only one # rating for a given subject is non-null in both test and train groups for a given # dataset, or variance is otherwise zero. if dataset == \"all\" : noNanMask = ( ~ np . isnan ( self . data )) & ( ~ np . isnan ( self . predictions )) for i in self . data . index : r . append ( pearsonr ( self . data . loc [ i , :][ noNanMask . loc [ i , :]], self . predictions . loc [ i , :][ noNanMask . loc [ i , :]], )[ 0 ] ) elif self . is_mask : if dataset == \"train\" : noNanMask = ( ~ np . isnan ( self . masked_data )) & ( ~ np . isnan ( self . predictions ) ) if self . is_mask_dilated : for i in self . masked_data . index : r . append ( pearsonr ( self . masked_data . loc [ i , self . dilated_mask . loc [ i , :]][ noNanMask . loc [ i , :] ], self . predictions . loc [ i , self . dilated_mask . loc [ i , :]][ noNanMask . loc [ i , :] ], )[ 0 ] ) else : for i in self . masked_data . index : r . append ( pearsonr ( self . masked_data . loc [ i , self . train_mask . loc [ i , :]][ noNanMask . loc [ i , :] ], self . predictions . loc [ i , self . train_mask . loc [ i , :]][ noNanMask . loc [ i , :] ], )[ 0 ] ) else : # test noNanMask = ( ~ np . isnan ( self . data )) & ( ~ np . isnan ( self . predictions )) for i in self . masked_data . index : r . append ( pearsonr ( self . data . loc [ i , ~ self . train_mask . loc [ i , :]][ noNanMask . loc [ i , :] ], self . predictions . loc [ i , ~ self . train_mask . loc [ i , :]][ noNanMask . loc [ i , :] ], )[ 0 ] ) else : raise ValueError ( \"Must run split_train_test() before using this option.\" ) return np . array ( r ) get_sub_mse ( self , dataset = 'test' ) Calculate observed/predicted mse for each subject in matrix Parameters: Name Type Description Default dataset str Get mse on 'all' data, the 'train' data, or the 'test' data 'test' Returns: Type Description mse (float) mean squared error Source code in emotioncf/base.py def get_sub_mse ( self , dataset = \"test\" ): \"\"\"Calculate observed/predicted mse for each subject in matrix Args: dataset (str): Get mse on 'all' data, the 'train' data, or the 'test' data Returns: mse (float): mean squared error \"\"\" if not self . is_fit : raise ValueError ( \"You must fit() model first before using this method.\" ) if not self . is_predict : raise ValueError ( \"You must predict() model first before using this method.\" ) mse = [] if dataset == \"all\" : for i in self . data . index : actual = self . data . loc [ i , :] pred = self . predictions . loc [ i , :] mse . append ( np . nanmean ( ( pred [( ~ np . isnan ( actual )) & ( ~ np . isnan ( pred ))] - actual [( ~ np . isnan ( actual )) & ( ~ np . isnan ( pred ))] ) ** 2 ) ) elif self . is_mask : if dataset == \"train\" : if self . is_mask_dilated : for i in self . masked_data . index : actual = self . masked_data . loc [ i , self . dilated_mask . loc [ i , :]] pred = self . predictions . loc [ i , self . dilated_mask . loc [ i , :]] mse . append ( np . nanmean ( ( pred [( ~ np . isnan ( actual )) & ( ~ np . isnan ( pred ))] - actual [( ~ np . isnan ( actual )) & ( ~ np . isnan ( pred ))] ) ** 2 ) ) else : for i in self . data . index : actual = self . masked_data . loc [ i , self . train_mask . loc [ i , :]] pred = self . predictions . loc [ i , self . train_mask . loc [ i , :]] mse . append ( np . nanmean ( ( pred [( ~ np . isnan ( actual )) & ( ~ np . isnan ( pred ))] - actual [( ~ np . isnan ( actual )) & ( ~ np . isnan ( pred ))] ) ** 2 ) ) else : for i in self . data . index : actual = self . data . loc [ i , ~ self . train_mask . loc [ i , :]] pred = self . predictions . loc [ i , ~ self . train_mask . loc [ i , :]] mse . append ( np . nanmean ( ( pred [( ~ np . isnan ( actual )) & ( ~ np . isnan ( pred ))] - actual [( ~ np . isnan ( actual )) & ( ~ np . isnan ( pred ))] ) ** 2 ) ) else : raise ValueError ( \"Must run split_train_test() before using this option.\" ) return np . array ( mse ) plot_predictions ( self , dataset = 'train' , verbose = True , heatmapkwargs = {}) Create plot of actual and predicted data Parameters: Name Type Description Default dataset str plot 'all' data, the 'train' data, or the 'test' data 'train' verbose bool; optional print the averaged subject correlation while plotting; Default True True Returns: Type Description r (float) Correlation Source code in emotioncf/base.py def plot_predictions ( self , dataset = \"train\" , verbose = True , heatmapkwargs = {}): \"\"\"Create plot of actual and predicted data Args: dataset (str): plot 'all' data, the 'train' data, or the 'test' data verbose (bool; optional): print the averaged subject correlation while plotting; Default True Returns: r (float): Correlation \"\"\" import matplotlib.pyplot as plt import seaborn as sns if not self . is_fit : raise ValueError ( \"You must fit() model first before using this method.\" ) if not self . is_predict : raise ValueError ( \"You must predict() model first before using this method.\" ) if self . is_mask : data = self . masked_data . copy () else : data = self . data . copy () heatmapkwargs . setdefault ( \"square\" , False ) heatmapkwargs . setdefault ( \"xticklabels\" , False ) heatmapkwargs . setdefault ( \"yticklabels\" , False ) vmax = ( data . max () . max () if data . max () . max () > self . predictions . max () . max () else self . predictions . max () . max () ) vmin = ( data . min () . min () if data . min () . min () < self . predictions . min () . min () else self . predictions . min () . min () ) heatmapkwargs . setdefault ( \"vmax\" , vmax ) heatmapkwargs . setdefault ( \"vmin\" , vmin ) f , ax = plt . subplots ( nrows = 1 , ncols = 3 , figsize = ( 15 , 8 )) sns . heatmap ( data , ax = ax [ 0 ], ** heatmapkwargs ) ax [ 0 ] . set_title ( \"Actual User/Item Ratings\" ) ax [ 0 ] . set_xlabel ( \"Items\" , fontsize = 18 ) ax [ 0 ] . set_ylabel ( \"Users\" , fontsize = 18 ) sns . heatmap ( self . predictions , ax = ax [ 1 ], ** heatmapkwargs ) ax [ 1 ] . set_title ( \"Predicted User/Item Ratings\" ) ax [ 1 ] . set_xlabel ( \"Items\" , fontsize = 18 ) ax [ 1 ] . set_ylabel ( \"Users\" , fontsize = 18 ) f . tight_layout () actual , pred = self . _retrieve_predictions ( dataset ) ax [ 2 ] . scatter ( actual [( ~ np . isnan ( actual )) & ( ~ np . isnan ( pred ))], pred [( ~ np . isnan ( actual )) & ( ~ np . isnan ( pred ))], ) ax [ 2 ] . set_xlabel ( \"Actual Ratings\" ) ax [ 2 ] . set_ylabel ( \"Predicted Ratings\" ) ax [ 2 ] . set_title ( \"Predicted Ratings\" ) r = self . get_sub_corr ( dataset = dataset ) . mean () if verbose : print ( \"Average Subject Correlation: %s \" % r ) return f , r split_train_test ( self , n_train_items = 0.1 ) Split data into training and testing sets Parameters: Name Type Description Default n_train_items int/float if an integer is passed its raw value is used. Otherwise if a float is passed its taken to be a (rounded) percentage of the total items; Default .1 (10% of the data) 0.1 Source code in emotioncf/base.py def split_train_test ( self , n_train_items = 0.1 ): \"\"\" Split data into training and testing sets Args: n_train_items (int/float, optional): if an integer is passed its raw value is used. Otherwise if a float is passed its taken to be a (rounded) percentage of the total items; Default .1 (10% of the data) \"\"\" if isinstance ( n_train_items , ( float , np . floating )) and 1 >= n_train_items > 0 : self . n_train_items = int ( np . round ( self . data . shape [ 1 ] * n_train_items )) elif isinstance ( n_train_items , ( int , np . integer )): self . n_train_items = n_train_items else : raise TypeError ( f \"n_train_items must be an integer or a float between 0-1, not { type ( n_train_items ) } with value { n_train_items } \" ) self . train_mask = self . data . copy () self . train_mask . loc [:, :] = np . zeros ( self . data . shape ) . astype ( bool ) for sub in self . data . index : sub_train_rating_item = np . random . choice ( self . data . columns , replace = False , size = self . n_train_items ) self . train_mask . loc [ sub , sub_train_rating_item ] = True self . masked_data = self . data [ self . train_mask ] self . is_mask = True to_long_df ( self ) Create a long format pandas dataframe with observed, predicted, and mask. Source code in emotioncf/base.py def to_long_df ( self ): \"\"\" Create a long format pandas dataframe with observed, predicted, and mask.\"\"\" observed = pd . DataFrame ( columns = [ \"Subject\" , \"Item\" , \"Rating\" , \"Condition\" ]) for row in self . data . iterrows (): tmp = pd . DataFrame ( columns = observed . columns ) tmp [ \"Rating\" ] = row [ 1 ] tmp [ \"Item\" ] = self . data . columns tmp [ \"Subject\" ] = row [ 0 ] tmp [ \"Condition\" ] = \"Observed\" if self . is_mask : if self . is_mask_dilated : tmp [ \"Mask\" ] = self . dilated_mask . loc [ row [ 0 ]] else : tmp [ \"Mask\" ] = self . train_mask . loc [ row [ 0 ]] observed = observed . append ( tmp ) if self . is_predict : predicted = pd . DataFrame ( columns = [ \"Subject\" , \"Item\" , \"Rating\" , \"Condition\" ]) for row in self . predictions . iterrows (): tmp = pd . DataFrame ( columns = predicted . columns ) tmp [ \"Rating\" ] = row [ 1 ] tmp [ \"Item\" ] = self . predictions . columns tmp [ \"Subject\" ] = row [ 0 ] tmp [ \"Condition\" ] = \"Predicted\" if self . is_mask : tmp [ \"Mask\" ] = self . train_mask . loc [ row [ 0 ]] predicted = predicted . append ( tmp ) observed = observed . append ( predicted ) return observed BaseNMF Base class for NMF algorithms. plot_learning ( self , save = False ) Plot training error over iterations for diagnostic purposes Parameters: Name Type Description Default save bool/str/Path if a string or path is provided will save the figure to that location. Defaults to False. False Returns: Type Description tuple (figure handle, axes handle) Source code in emotioncf/base.py def plot_learning ( self , save = False ): \"\"\" Plot training error over iterations for diagnostic purposes Args: save (bool/str/Path, optional): if a string or path is provided will save the figure to that location. Defaults to False. Returns: tuple: (figure handle, axes handle) \"\"\" if self . is_fit : if len ( self . error_history ) > 0 : f , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 6 )) _ = ax . plot ( range ( 1 , len ( self . error_history ) + 1 ), self . error_history ) ax . set ( xlabel = \"Iteration\" , ylabel = \"Normalized Error\" , title = f \"Final Normalized Error: { np . round ( self . _norm_e , 3 ) } \\n Converged: { self . converged } \" , ) if save : plt . savefig ( save , bbox_inches = \"tight\" ) return f , ax else : raise ValueError ( \"Learning was not saved during fit. save_learning was False\" ) else : raise ValueError ( \"Model has not been fit.\" )","title":"Base"},{"location":"api/base/#emotioncfbase","text":"","title":"emotioncf.base"},{"location":"api/base/#emotioncf.base","text":"Base algorithm classes. All other algorithms inherit from these classes which means they have access to all their methods and attributes. You won't typically utilize these classes directly unless you're creating a custom estimator.","title":"emotioncf.base"},{"location":"api/base/#emotioncf.base.Base","text":"All other models and base classes inherit from this class.","title":"Base"},{"location":"api/base/#emotioncf.base.Base.downsample","text":"Downsample rating matrix to a new target frequency or number of samples using averaging. Parameters: Name Type Description Default sampling_freq int/float Sampling frequency of data None target int/float downsampling target None target_type str type of target can be [samples,seconds,hz] 'samples' Source code in emotioncf/base.py def downsample ( self , sampling_freq = None , target = None , target_type = \"samples\" ): \"\"\"Downsample rating matrix to a new target frequency or number of samples using averaging. Args: sampling_freq (int/float): Sampling frequency of data target (int/float): downsampling target target_type (str): type of target can be [samples,seconds,hz] \"\"\" if sampling_freq is None : raise ValueError ( \"Please specify the sampling frequency of the data.\" ) if target is None : raise ValueError ( \"Please specify the downsampling target.\" ) if target_type is None : raise ValueError ( \"Please specify the type of target to downsample to [samples,seconds,hz].\" ) def ds ( data , sampling_freq = sampling_freq , target = None , target_type = \"samples\" ): if target_type == \"samples\" : n_samples = target elif target_type == \"seconds\" : n_samples = target * sampling_freq elif target_type == \"hz\" : n_samples = sampling_freq / target else : raise ValueError ( 'Make sure target_type is \"samples\", \"seconds\", or \"hz\".' ) data = data . T idx = np . sort ( np . repeat ( np . arange ( 1 , data . shape [ 0 ] / n_samples , 1 ), n_samples ) ) if data . shape [ 0 ] > len ( idx ): idx = np . concatenate ( [ idx , np . repeat ( idx [ - 1 ] + 1 , data . shape [ 0 ] - len ( idx ))] ) return data . groupby ( idx ) . mean () . T self . data = ds ( self . data , sampling_freq = sampling_freq , target = target , target_type = target_type , ) if self . is_mask : self . train_mask = ds ( self . train_mask , sampling_freq = sampling_freq , target = target , target_type = target_type , ) self . train_mask . loc [:, :] = self . train_mask > 0 self . masked_data = ds ( self . masked_data , sampling_freq = sampling_freq , target = target , target_type = target_type , ) if self . is_mask_dilated : self . dilated_mask = ds ( self . dilated_mask , sampling_freq = sampling_freq , target = target , target_type = target_type , ) self . dilated_mask . loc [:, :] = self . dilated_mask > 0 if self . is_predict : self . predictions = ds ( self . predictions , sampling_freq = sampling_freq , target = target , target_type = target_type , )","title":"downsample()"},{"location":"api/base/#emotioncf.base.Base.get_corr","text":"Get overall correlation for predicted compared to actual for all items and subjects. Parameters: Name Type Description Default dataset str Get correlation on 'all' data, the 'train' data, or the 'test' data 'test' Returns: Type Description r (float) Correlation Source code in emotioncf/base.py def get_corr ( self , dataset = \"test\" ): \"\"\"Get overall correlation for predicted compared to actual for all items and subjects. Args: dataset (str): Get correlation on 'all' data, the 'train' data, or the 'test' data Returns: r (float): Correlation \"\"\" if not self . is_fit : raise ValueError ( \"You must fit() model first before using this method.\" ) if not self . is_predict : raise ValueError ( \"You must predict() model first before using this method.\" ) actual , pred = self . _retrieve_predictions ( dataset ) # Handle nans when computing correlation nans = np . logical_or ( np . isnan ( actual ), np . isnan ( pred )) return pearsonr ( actual [ ~ nans ], pred [ ~ nans ])[ 0 ]","title":"get_corr()"},{"location":"api/base/#emotioncf.base.Base.get_mse","text":"Get overall mean squared error for predicted compared to actual for all items and subjects. Parameters: Name Type Description Default dataset str Get mse on 'all' data, the 'train' data, or the 'test' data 'test' Returns: Type Description mse (float) mean squared error Source code in emotioncf/base.py def get_mse ( self , dataset = \"test\" ): \"\"\"Get overall mean squared error for predicted compared to actual for all items and subjects. Args: dataset (str): Get mse on 'all' data, the 'train' data, or the 'test' data Returns: mse (float): mean squared error \"\"\" if not self . is_fit : raise ValueError ( \"You must fit() model first before using this method.\" ) if not self . is_predict : raise ValueError ( \"You must predict() model first before using this method.\" ) actual , pred = self . _retrieve_predictions ( dataset ) return np . nanmean (( pred - actual ) ** 2 )","title":"get_mse()"},{"location":"api/base/#emotioncf.base.Base.get_sub_corr","text":"Calculate observed/predicted correlation for each subject in matrix Parameters: Name Type Description Default dataset str Get correlation on 'all' data, the 'train' data, or the 'test' data 'test' Returns: Type Description r (float) Correlation Source code in emotioncf/base.py def get_sub_corr ( self , dataset = \"test\" ): \"\"\"Calculate observed/predicted correlation for each subject in matrix Args: dataset (str): Get correlation on 'all' data, the 'train' data, or the 'test' data Returns: r (float): Correlation \"\"\" if not self . is_fit : raise ValueError ( \"You must fit() model first before using this method.\" ) if not self . is_predict : raise ValueError ( \"You must predict() model first before using this method.\" ) r = [] # Note: the following mask prevents NaN values from being passed to `pearsonr()`. # However, it does not guaratee that no correlation values will be NaN, e.g. if only one # rating for a given subject is non-null in both test and train groups for a given # dataset, or variance is otherwise zero. if dataset == \"all\" : noNanMask = ( ~ np . isnan ( self . data )) & ( ~ np . isnan ( self . predictions )) for i in self . data . index : r . append ( pearsonr ( self . data . loc [ i , :][ noNanMask . loc [ i , :]], self . predictions . loc [ i , :][ noNanMask . loc [ i , :]], )[ 0 ] ) elif self . is_mask : if dataset == \"train\" : noNanMask = ( ~ np . isnan ( self . masked_data )) & ( ~ np . isnan ( self . predictions ) ) if self . is_mask_dilated : for i in self . masked_data . index : r . append ( pearsonr ( self . masked_data . loc [ i , self . dilated_mask . loc [ i , :]][ noNanMask . loc [ i , :] ], self . predictions . loc [ i , self . dilated_mask . loc [ i , :]][ noNanMask . loc [ i , :] ], )[ 0 ] ) else : for i in self . masked_data . index : r . append ( pearsonr ( self . masked_data . loc [ i , self . train_mask . loc [ i , :]][ noNanMask . loc [ i , :] ], self . predictions . loc [ i , self . train_mask . loc [ i , :]][ noNanMask . loc [ i , :] ], )[ 0 ] ) else : # test noNanMask = ( ~ np . isnan ( self . data )) & ( ~ np . isnan ( self . predictions )) for i in self . masked_data . index : r . append ( pearsonr ( self . data . loc [ i , ~ self . train_mask . loc [ i , :]][ noNanMask . loc [ i , :] ], self . predictions . loc [ i , ~ self . train_mask . loc [ i , :]][ noNanMask . loc [ i , :] ], )[ 0 ] ) else : raise ValueError ( \"Must run split_train_test() before using this option.\" ) return np . array ( r )","title":"get_sub_corr()"},{"location":"api/base/#emotioncf.base.Base.get_sub_mse","text":"Calculate observed/predicted mse for each subject in matrix Parameters: Name Type Description Default dataset str Get mse on 'all' data, the 'train' data, or the 'test' data 'test' Returns: Type Description mse (float) mean squared error Source code in emotioncf/base.py def get_sub_mse ( self , dataset = \"test\" ): \"\"\"Calculate observed/predicted mse for each subject in matrix Args: dataset (str): Get mse on 'all' data, the 'train' data, or the 'test' data Returns: mse (float): mean squared error \"\"\" if not self . is_fit : raise ValueError ( \"You must fit() model first before using this method.\" ) if not self . is_predict : raise ValueError ( \"You must predict() model first before using this method.\" ) mse = [] if dataset == \"all\" : for i in self . data . index : actual = self . data . loc [ i , :] pred = self . predictions . loc [ i , :] mse . append ( np . nanmean ( ( pred [( ~ np . isnan ( actual )) & ( ~ np . isnan ( pred ))] - actual [( ~ np . isnan ( actual )) & ( ~ np . isnan ( pred ))] ) ** 2 ) ) elif self . is_mask : if dataset == \"train\" : if self . is_mask_dilated : for i in self . masked_data . index : actual = self . masked_data . loc [ i , self . dilated_mask . loc [ i , :]] pred = self . predictions . loc [ i , self . dilated_mask . loc [ i , :]] mse . append ( np . nanmean ( ( pred [( ~ np . isnan ( actual )) & ( ~ np . isnan ( pred ))] - actual [( ~ np . isnan ( actual )) & ( ~ np . isnan ( pred ))] ) ** 2 ) ) else : for i in self . data . index : actual = self . masked_data . loc [ i , self . train_mask . loc [ i , :]] pred = self . predictions . loc [ i , self . train_mask . loc [ i , :]] mse . append ( np . nanmean ( ( pred [( ~ np . isnan ( actual )) & ( ~ np . isnan ( pred ))] - actual [( ~ np . isnan ( actual )) & ( ~ np . isnan ( pred ))] ) ** 2 ) ) else : for i in self . data . index : actual = self . data . loc [ i , ~ self . train_mask . loc [ i , :]] pred = self . predictions . loc [ i , ~ self . train_mask . loc [ i , :]] mse . append ( np . nanmean ( ( pred [( ~ np . isnan ( actual )) & ( ~ np . isnan ( pred ))] - actual [( ~ np . isnan ( actual )) & ( ~ np . isnan ( pred ))] ) ** 2 ) ) else : raise ValueError ( \"Must run split_train_test() before using this option.\" ) return np . array ( mse )","title":"get_sub_mse()"},{"location":"api/base/#emotioncf.base.Base.plot_predictions","text":"Create plot of actual and predicted data Parameters: Name Type Description Default dataset str plot 'all' data, the 'train' data, or the 'test' data 'train' verbose bool; optional print the averaged subject correlation while plotting; Default True True Returns: Type Description r (float) Correlation Source code in emotioncf/base.py def plot_predictions ( self , dataset = \"train\" , verbose = True , heatmapkwargs = {}): \"\"\"Create plot of actual and predicted data Args: dataset (str): plot 'all' data, the 'train' data, or the 'test' data verbose (bool; optional): print the averaged subject correlation while plotting; Default True Returns: r (float): Correlation \"\"\" import matplotlib.pyplot as plt import seaborn as sns if not self . is_fit : raise ValueError ( \"You must fit() model first before using this method.\" ) if not self . is_predict : raise ValueError ( \"You must predict() model first before using this method.\" ) if self . is_mask : data = self . masked_data . copy () else : data = self . data . copy () heatmapkwargs . setdefault ( \"square\" , False ) heatmapkwargs . setdefault ( \"xticklabels\" , False ) heatmapkwargs . setdefault ( \"yticklabels\" , False ) vmax = ( data . max () . max () if data . max () . max () > self . predictions . max () . max () else self . predictions . max () . max () ) vmin = ( data . min () . min () if data . min () . min () < self . predictions . min () . min () else self . predictions . min () . min () ) heatmapkwargs . setdefault ( \"vmax\" , vmax ) heatmapkwargs . setdefault ( \"vmin\" , vmin ) f , ax = plt . subplots ( nrows = 1 , ncols = 3 , figsize = ( 15 , 8 )) sns . heatmap ( data , ax = ax [ 0 ], ** heatmapkwargs ) ax [ 0 ] . set_title ( \"Actual User/Item Ratings\" ) ax [ 0 ] . set_xlabel ( \"Items\" , fontsize = 18 ) ax [ 0 ] . set_ylabel ( \"Users\" , fontsize = 18 ) sns . heatmap ( self . predictions , ax = ax [ 1 ], ** heatmapkwargs ) ax [ 1 ] . set_title ( \"Predicted User/Item Ratings\" ) ax [ 1 ] . set_xlabel ( \"Items\" , fontsize = 18 ) ax [ 1 ] . set_ylabel ( \"Users\" , fontsize = 18 ) f . tight_layout () actual , pred = self . _retrieve_predictions ( dataset ) ax [ 2 ] . scatter ( actual [( ~ np . isnan ( actual )) & ( ~ np . isnan ( pred ))], pred [( ~ np . isnan ( actual )) & ( ~ np . isnan ( pred ))], ) ax [ 2 ] . set_xlabel ( \"Actual Ratings\" ) ax [ 2 ] . set_ylabel ( \"Predicted Ratings\" ) ax [ 2 ] . set_title ( \"Predicted Ratings\" ) r = self . get_sub_corr ( dataset = dataset ) . mean () if verbose : print ( \"Average Subject Correlation: %s \" % r ) return f , r","title":"plot_predictions()"},{"location":"api/base/#emotioncf.base.Base.split_train_test","text":"Split data into training and testing sets Parameters: Name Type Description Default n_train_items int/float if an integer is passed its raw value is used. Otherwise if a float is passed its taken to be a (rounded) percentage of the total items; Default .1 (10% of the data) 0.1 Source code in emotioncf/base.py def split_train_test ( self , n_train_items = 0.1 ): \"\"\" Split data into training and testing sets Args: n_train_items (int/float, optional): if an integer is passed its raw value is used. Otherwise if a float is passed its taken to be a (rounded) percentage of the total items; Default .1 (10% of the data) \"\"\" if isinstance ( n_train_items , ( float , np . floating )) and 1 >= n_train_items > 0 : self . n_train_items = int ( np . round ( self . data . shape [ 1 ] * n_train_items )) elif isinstance ( n_train_items , ( int , np . integer )): self . n_train_items = n_train_items else : raise TypeError ( f \"n_train_items must be an integer or a float between 0-1, not { type ( n_train_items ) } with value { n_train_items } \" ) self . train_mask = self . data . copy () self . train_mask . loc [:, :] = np . zeros ( self . data . shape ) . astype ( bool ) for sub in self . data . index : sub_train_rating_item = np . random . choice ( self . data . columns , replace = False , size = self . n_train_items ) self . train_mask . loc [ sub , sub_train_rating_item ] = True self . masked_data = self . data [ self . train_mask ] self . is_mask = True","title":"split_train_test()"},{"location":"api/base/#emotioncf.base.Base.to_long_df","text":"Create a long format pandas dataframe with observed, predicted, and mask. Source code in emotioncf/base.py def to_long_df ( self ): \"\"\" Create a long format pandas dataframe with observed, predicted, and mask.\"\"\" observed = pd . DataFrame ( columns = [ \"Subject\" , \"Item\" , \"Rating\" , \"Condition\" ]) for row in self . data . iterrows (): tmp = pd . DataFrame ( columns = observed . columns ) tmp [ \"Rating\" ] = row [ 1 ] tmp [ \"Item\" ] = self . data . columns tmp [ \"Subject\" ] = row [ 0 ] tmp [ \"Condition\" ] = \"Observed\" if self . is_mask : if self . is_mask_dilated : tmp [ \"Mask\" ] = self . dilated_mask . loc [ row [ 0 ]] else : tmp [ \"Mask\" ] = self . train_mask . loc [ row [ 0 ]] observed = observed . append ( tmp ) if self . is_predict : predicted = pd . DataFrame ( columns = [ \"Subject\" , \"Item\" , \"Rating\" , \"Condition\" ]) for row in self . predictions . iterrows (): tmp = pd . DataFrame ( columns = predicted . columns ) tmp [ \"Rating\" ] = row [ 1 ] tmp [ \"Item\" ] = self . predictions . columns tmp [ \"Subject\" ] = row [ 0 ] tmp [ \"Condition\" ] = \"Predicted\" if self . is_mask : tmp [ \"Mask\" ] = self . train_mask . loc [ row [ 0 ]] predicted = predicted . append ( tmp ) observed = observed . append ( predicted ) return observed","title":"to_long_df()"},{"location":"api/base/#emotioncf.base.BaseNMF","text":"Base class for NMF algorithms.","title":"BaseNMF"},{"location":"api/base/#emotioncf.base.BaseNMF.plot_learning","text":"Plot training error over iterations for diagnostic purposes Parameters: Name Type Description Default save bool/str/Path if a string or path is provided will save the figure to that location. Defaults to False. False Returns: Type Description tuple (figure handle, axes handle) Source code in emotioncf/base.py def plot_learning ( self , save = False ): \"\"\" Plot training error over iterations for diagnostic purposes Args: save (bool/str/Path, optional): if a string or path is provided will save the figure to that location. Defaults to False. Returns: tuple: (figure handle, axes handle) \"\"\" if self . is_fit : if len ( self . error_history ) > 0 : f , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 6 )) _ = ax . plot ( range ( 1 , len ( self . error_history ) + 1 ), self . error_history ) ax . set ( xlabel = \"Iteration\" , ylabel = \"Normalized Error\" , title = f \"Final Normalized Error: { np . round ( self . _norm_e , 3 ) } \\n Converged: { self . converged } \" , ) if save : plt . savefig ( save , bbox_inches = \"tight\" ) return f , ax else : raise ValueError ( \"Learning was not saved during fit. save_learning was False\" ) else : raise ValueError ( \"Model has not been fit.\" )","title":"plot_learning()"},{"location":"api/knn/","text":"emotioncf.models.KNN The K-Nearest Neighbors algorithm makes predictions using a weighted mean of a subset of similar users. Similarity can be controlled via the metric argument to the .fit method, and the number of other users can be controlled with the k argument to the .predict method. fit ( self , metric = 'pearson' , dilate_ts_n_samples = None , ** kwargs ) Fit collaborative model to train data. Calculate similarity between subjects across items Parameters: Name Type Description Default metric str; optional type of similarity. One of 'pearson', 'spearman', 'kendall', 'cosine', or 'correlation'. 'correlation' is just an alias for 'pearson'. Default 'pearson'. 'pearson' Source code in emotioncf/models.py def fit ( self , metric = \"pearson\" , dilate_ts_n_samples = None , ** kwargs ): \"\"\"Fit collaborative model to train data. Calculate similarity between subjects across items Args: metric (str; optional): type of similarity. One of 'pearson', 'spearman', 'kendall', 'cosine', or 'correlation'. 'correlation' is just an alias for 'pearson'. Default 'pearson'. \"\"\" metrics = [ \"pearson\" , \"spearman\" , \"kendall\" , \"cosine\" , \"correlation\" ] if metric not in metrics : raise ValueError ( f \"metric must be one of { metrics } \" ) if self . is_mask : data = self . data [ self . train_mask ] else : data = self . data . copy () if dilate_ts_n_samples is not None : data = self . _dilate_ts_rating_samples ( n_samples = dilate_ts_n_samples ) data = data [ self . dilated_mask ] if metric in [ \"pearson\" , \"kendall\" , \"spearman\" ]: # Fall back to pandas sim = data . T . corr ( method = metric ) else : sim = pd . DataFrame ( 1 - nanpdist ( data . to_numpy (), metric = metric ), index = data . index , columns = data . index , ) self . subject_similarity = sim self . is_fit = True predict ( self , k = None , ** kwargs ) Predict Subject's missing items using similarity based collaborative filtering. Parameters: Name Type Description Default k int number of closest neighbors to use None Returns: Type Description predicted_rating (Dataframe) adds field to object instance Source code in emotioncf/models.py def predict ( self , k = None , ** kwargs ): \"\"\"Predict Subject's missing items using similarity based collaborative filtering. Args: k (int): number of closest neighbors to use Returns: predicted_rating (Dataframe): adds field to object instance \"\"\" if not self . is_fit : raise ValueError ( \"You must fit() model first before using this method.\" ) if self . is_mask : data = self . masked_data . copy () else : data = self . data . copy () pred = pd . DataFrame ( np . zeros ( data . shape )) pred . columns = data . columns pred . index = data . index for row in data . iterrows (): if k is not None : top_subjects = ( self . subject_similarity . loc [ row [ 0 ]] . drop ( row [ 0 ]) . sort_values ( ascending = False )[ 0 : k ] ) else : top_subjects = ( self . subject_similarity . loc [ row [ 0 ]] . drop ( row [ 0 ]) . sort_values ( ascending = False ) ) top_subjects = top_subjects [ ~ top_subjects . isnull ()] # remove nan subjects for col in data . iteritems (): pred . loc [ row [ 0 ], col [ 0 ]] = np . dot ( top_subjects , self . data . loc [ top_subjects . index , col [ 0 ]] . T ) / len ( top_subjects ) self . predictions = pred self . is_predict = True","title":"KNN"},{"location":"api/knn/#emotioncfmodelsknn","text":"","title":"emotioncf.models.KNN"},{"location":"api/knn/#emotioncf.models.KNN","text":"The K-Nearest Neighbors algorithm makes predictions using a weighted mean of a subset of similar users. Similarity can be controlled via the metric argument to the .fit method, and the number of other users can be controlled with the k argument to the .predict method.","title":"emotioncf.models.KNN"},{"location":"api/knn/#emotioncf.models.KNN.fit","text":"Fit collaborative model to train data. Calculate similarity between subjects across items Parameters: Name Type Description Default metric str; optional type of similarity. One of 'pearson', 'spearman', 'kendall', 'cosine', or 'correlation'. 'correlation' is just an alias for 'pearson'. Default 'pearson'. 'pearson' Source code in emotioncf/models.py def fit ( self , metric = \"pearson\" , dilate_ts_n_samples = None , ** kwargs ): \"\"\"Fit collaborative model to train data. Calculate similarity between subjects across items Args: metric (str; optional): type of similarity. One of 'pearson', 'spearman', 'kendall', 'cosine', or 'correlation'. 'correlation' is just an alias for 'pearson'. Default 'pearson'. \"\"\" metrics = [ \"pearson\" , \"spearman\" , \"kendall\" , \"cosine\" , \"correlation\" ] if metric not in metrics : raise ValueError ( f \"metric must be one of { metrics } \" ) if self . is_mask : data = self . data [ self . train_mask ] else : data = self . data . copy () if dilate_ts_n_samples is not None : data = self . _dilate_ts_rating_samples ( n_samples = dilate_ts_n_samples ) data = data [ self . dilated_mask ] if metric in [ \"pearson\" , \"kendall\" , \"spearman\" ]: # Fall back to pandas sim = data . T . corr ( method = metric ) else : sim = pd . DataFrame ( 1 - nanpdist ( data . to_numpy (), metric = metric ), index = data . index , columns = data . index , ) self . subject_similarity = sim self . is_fit = True","title":"fit()"},{"location":"api/knn/#emotioncf.models.KNN.predict","text":"Predict Subject's missing items using similarity based collaborative filtering. Parameters: Name Type Description Default k int number of closest neighbors to use None Returns: Type Description predicted_rating (Dataframe) adds field to object instance Source code in emotioncf/models.py def predict ( self , k = None , ** kwargs ): \"\"\"Predict Subject's missing items using similarity based collaborative filtering. Args: k (int): number of closest neighbors to use Returns: predicted_rating (Dataframe): adds field to object instance \"\"\" if not self . is_fit : raise ValueError ( \"You must fit() model first before using this method.\" ) if self . is_mask : data = self . masked_data . copy () else : data = self . data . copy () pred = pd . DataFrame ( np . zeros ( data . shape )) pred . columns = data . columns pred . index = data . index for row in data . iterrows (): if k is not None : top_subjects = ( self . subject_similarity . loc [ row [ 0 ]] . drop ( row [ 0 ]) . sort_values ( ascending = False )[ 0 : k ] ) else : top_subjects = ( self . subject_similarity . loc [ row [ 0 ]] . drop ( row [ 0 ]) . sort_values ( ascending = False ) ) top_subjects = top_subjects [ ~ top_subjects . isnull ()] # remove nan subjects for col in data . iteritems (): pred . loc [ row [ 0 ], col [ 0 ]] = np . dot ( top_subjects , self . data . loc [ top_subjects . index , col [ 0 ]] . T ) / len ( top_subjects ) self . predictions = pred self . is_predict = True","title":"predict()"},{"location":"api/mean/","text":"emotioncf.models.Mean The Mean algorithm simply uses the mean of other users to make predictions about items. It's primarily useful as a good baseline model. fit ( self , dilate_ts_n_samples = None , ** kwargs ) Fit collaborative model to train data. Calculate similarity between subjects across items Parameters: Name Type Description Default dilate_ts_n_samples int will dilate masked samples by n_samples to leverage auto-correlation in estimating time-series data None Source code in emotioncf/models.py def fit ( self , dilate_ts_n_samples = None , ** kwargs ): \"\"\"Fit collaborative model to train data. Calculate similarity between subjects across items Args: dilate_ts_n_samples (int): will dilate masked samples by n_samples to leverage auto-correlation in estimating time-series data \"\"\" if self . is_mask : if dilate_ts_n_samples is not None : _ = self . _dilate_ts_rating_samples ( n_samples = dilate_ts_n_samples ) self . mean = self . masked_data [ self . dilated_mask ] . mean ( skipna = True , axis = 0 ) else : self . mean = self . masked_data [ self . train_mask ] . mean ( skipna = True , axis = 0 ) else : self . mean = self . data . mean ( skipna = True , axis = 0 ) self . is_fit = True predict ( self ) Predict missing items using other subject's item means. Returns: Type Description predicted_rating (Dataframe) adds field to object instance Source code in emotioncf/models.py def predict ( self ): \"\"\"Predict missing items using other subject's item means. Returns: predicted_rating (Dataframe): adds field to object instance \"\"\" if not self . is_fit : raise ValueError ( \"You must fit() model first before using this method.\" ) self . predictions = self . data . copy () for row in self . data . iterrows (): self . predictions . loc [ row [ 0 ]] = self . mean self . is_predict = True","title":"Mean"},{"location":"api/mean/#emotioncfmodelsmean","text":"","title":"emotioncf.models.Mean"},{"location":"api/mean/#emotioncf.models.Mean","text":"The Mean algorithm simply uses the mean of other users to make predictions about items. It's primarily useful as a good baseline model.","title":"emotioncf.models.Mean"},{"location":"api/mean/#emotioncf.models.Mean.fit","text":"Fit collaborative model to train data. Calculate similarity between subjects across items Parameters: Name Type Description Default dilate_ts_n_samples int will dilate masked samples by n_samples to leverage auto-correlation in estimating time-series data None Source code in emotioncf/models.py def fit ( self , dilate_ts_n_samples = None , ** kwargs ): \"\"\"Fit collaborative model to train data. Calculate similarity between subjects across items Args: dilate_ts_n_samples (int): will dilate masked samples by n_samples to leverage auto-correlation in estimating time-series data \"\"\" if self . is_mask : if dilate_ts_n_samples is not None : _ = self . _dilate_ts_rating_samples ( n_samples = dilate_ts_n_samples ) self . mean = self . masked_data [ self . dilated_mask ] . mean ( skipna = True , axis = 0 ) else : self . mean = self . masked_data [ self . train_mask ] . mean ( skipna = True , axis = 0 ) else : self . mean = self . data . mean ( skipna = True , axis = 0 ) self . is_fit = True","title":"fit()"},{"location":"api/mean/#emotioncf.models.Mean.predict","text":"Predict missing items using other subject's item means. Returns: Type Description predicted_rating (Dataframe) adds field to object instance Source code in emotioncf/models.py def predict ( self ): \"\"\"Predict missing items using other subject's item means. Returns: predicted_rating (Dataframe): adds field to object instance \"\"\" if not self . is_fit : raise ValueError ( \"You must fit() model first before using this method.\" ) self . predictions = self . data . copy () for row in self . data . iterrows (): self . predictions . loc [ row [ 0 ]] = self . mean self . is_predict = True","title":"predict()"},{"location":"api/nmf_m/","text":"emotioncf.models.NNMF_mult The Non-negative Matrix Factorization algorithm tries to decompose a users x items matrix into two additional matrices: users x factors and factors x items. Training is performed via multiplicative updating and continues until convergence or the maximum number of training iterations has been reached. The number of factors, convergence, and maximum iterations can be controlled with the n_factors , tol , and max_iterations arguments to the .fit method. By default the number of factors = the number items. fit ( self , n_factors = None , max_iterations = 100 , fit_error_limit = 1e-06 , error_limit = 1e-06 , verbose = False , dilate_ts_n_samples = None , save_learning = True , ** kwargs ) Fit NNMF collaborative filtering model to train data using multiplicative updating. Parameters: Name Type Description Default n_factors int Number of factors or components None max_iterations int maximum number of interations (default=100) 100 error_limit float error tolerance (default=1e-6) 1e-06 fit_error_limit float fit error tolerance (default=1e-6) 1e-06 verbose bool verbose output during fitting procedure (default=True) False dilate_ts_n_samples int will dilate masked samples by n_samples to leverage auto-correlation in estimating time-series data None save_learning bool; optional save a list of the error rate over iterations; Default True True Source code in emotioncf/models.py def fit ( self , n_factors = None , max_iterations = 100 , fit_error_limit = 1e-6 , error_limit = 1e-6 , verbose = False , dilate_ts_n_samples = None , save_learning = True , ** kwargs , ): \"\"\"Fit NNMF collaborative filtering model to train data using multiplicative updating. Args: n_factors (int): Number of factors or components max_iterations (int): maximum number of interations (default=100) error_limit (float): error tolerance (default=1e-6) fit_error_limit (float): fit error tolerance (default=1e-6) verbose (bool): verbose output during fitting procedure (default=True) dilate_ts_n_samples (int): will dilate masked samples by n_samples to leverage auto-correlation in estimating time-series data save_learning (bool; optional): save a list of the error rate over iterations; Default True \"\"\" eps = 1e-5 n_users , n_items = self . data . shape if n_factors is None : n_factors = n_items self . n_factors = n_factors # Initial guesses for solving X ~= WH. H is random [0,1] scaled by sqrt(X.mean() / n_factors) avg = np . sqrt ( np . nanmean ( self . data ) / n_factors ) self . H = avg * np . random . rand ( n_items , n_factors ) # H = Y self . W = avg * np . random . rand ( n_users , n_factors ) # W = A if self . is_mask : if dilate_ts_n_samples is not None : masked_X = self . _dilate_ts_rating_samples ( n_samples = dilate_ts_n_samples ) . values mask = self . dilated_mask . values else : mask = self . train_mask . values masked_X = self . data . values * mask masked_X [ np . isnan ( masked_X )] = 0 else : masked_X = self . data . values mask = np . ones ( self . data . shape ) X_est_prev = np . dot ( self . W , self . H ) ctr = 1 # TODO change to np.inf but make sure it doesn't screw up < fit_error_limit # TODO: Go over matrix math below, something is up with it cause n_factors doesn't work fit_residual = - np . inf current_resid = - np . inf error_limit = error_limit self . error_history = [] while ( ctr <= max_iterations or current_resid < error_limit or fit_residual < fit_error_limit ): # Update W: A=A.*(((W.*X)*Y')./((W.*(A*Y))*Y')); self . W *= np . dot ( masked_X , self . H . T ) / np . dot ( mask * np . dot ( self . W , self . H ), self . H . T ) self . W = np . maximum ( self . W , eps ) # Update H: Matlab: Y=Y.*((A'*(W.*X))./(A'*(W.*(A*Y)))); self . H *= np . dot ( self . W . T , masked_X ) / np . dot ( self . W . T , mask * np . dot ( self . W , self . H ) ) self . H = np . maximum ( self . H , eps ) # Evaluate X_est = np . dot ( self . W , self . H ) # This is basically error gradient for convergence purposes err = mask * ( X_est_prev - X_est ) fit_residual = np . sqrt ( np . sum ( err ** 2 )) # Save this iteration's predictions X_est_prev = X_est # Update the residuals; note that the norm = RMSE not MSE current_resid = np . linalg . norm ( masked_X - mask * X_est , ord = \"fro\" ) # Norm the residual with respect to the max of the dataset so we can use a common convergence threshold current_resid /= masked_X . max () if save_learning : self . error_history . append ( current_resid ) # curRes = linalg.norm(mask * (masked_X - X_est), ord='fro') if ctr % 10 == 0 and verbose : print ( \" \\t Current Iteration {} :\" . format ( ctr )) print ( \" \\t fit residual\" , np . round ( fit_residual , 4 )) ctr += 1 self . is_fit = True predict ( self ) Predict Subject's missing items using NNMF with multiplicative updating Returns: Type Description predicted_rating (Dataframe) adds field to object instance Source code in emotioncf/models.py def predict ( self ): \"\"\"Predict Subject's missing items using NNMF with multiplicative updating Returns: predicted_rating (Dataframe): adds field to object instance \"\"\" if not self . is_fit : raise ValueError ( \"You must fit() model first before using this method.\" ) self . predictions = self . data . copy () self . predictions . loc [:, :] = np . dot ( self . W , self . H ) self . is_predict = True","title":"NNMF_mult"},{"location":"api/nmf_m/#emotioncfmodelsnnmf_mult","text":"","title":"emotioncf.models.NNMF_mult"},{"location":"api/nmf_m/#emotioncf.models.NNMF_mult","text":"The Non-negative Matrix Factorization algorithm tries to decompose a users x items matrix into two additional matrices: users x factors and factors x items. Training is performed via multiplicative updating and continues until convergence or the maximum number of training iterations has been reached. The number of factors, convergence, and maximum iterations can be controlled with the n_factors , tol , and max_iterations arguments to the .fit method. By default the number of factors = the number items.","title":"emotioncf.models.NNMF_mult"},{"location":"api/nmf_m/#emotioncf.models.NNMF_mult.fit","text":"Fit NNMF collaborative filtering model to train data using multiplicative updating. Parameters: Name Type Description Default n_factors int Number of factors or components None max_iterations int maximum number of interations (default=100) 100 error_limit float error tolerance (default=1e-6) 1e-06 fit_error_limit float fit error tolerance (default=1e-6) 1e-06 verbose bool verbose output during fitting procedure (default=True) False dilate_ts_n_samples int will dilate masked samples by n_samples to leverage auto-correlation in estimating time-series data None save_learning bool; optional save a list of the error rate over iterations; Default True True Source code in emotioncf/models.py def fit ( self , n_factors = None , max_iterations = 100 , fit_error_limit = 1e-6 , error_limit = 1e-6 , verbose = False , dilate_ts_n_samples = None , save_learning = True , ** kwargs , ): \"\"\"Fit NNMF collaborative filtering model to train data using multiplicative updating. Args: n_factors (int): Number of factors or components max_iterations (int): maximum number of interations (default=100) error_limit (float): error tolerance (default=1e-6) fit_error_limit (float): fit error tolerance (default=1e-6) verbose (bool): verbose output during fitting procedure (default=True) dilate_ts_n_samples (int): will dilate masked samples by n_samples to leverage auto-correlation in estimating time-series data save_learning (bool; optional): save a list of the error rate over iterations; Default True \"\"\" eps = 1e-5 n_users , n_items = self . data . shape if n_factors is None : n_factors = n_items self . n_factors = n_factors # Initial guesses for solving X ~= WH. H is random [0,1] scaled by sqrt(X.mean() / n_factors) avg = np . sqrt ( np . nanmean ( self . data ) / n_factors ) self . H = avg * np . random . rand ( n_items , n_factors ) # H = Y self . W = avg * np . random . rand ( n_users , n_factors ) # W = A if self . is_mask : if dilate_ts_n_samples is not None : masked_X = self . _dilate_ts_rating_samples ( n_samples = dilate_ts_n_samples ) . values mask = self . dilated_mask . values else : mask = self . train_mask . values masked_X = self . data . values * mask masked_X [ np . isnan ( masked_X )] = 0 else : masked_X = self . data . values mask = np . ones ( self . data . shape ) X_est_prev = np . dot ( self . W , self . H ) ctr = 1 # TODO change to np.inf but make sure it doesn't screw up < fit_error_limit # TODO: Go over matrix math below, something is up with it cause n_factors doesn't work fit_residual = - np . inf current_resid = - np . inf error_limit = error_limit self . error_history = [] while ( ctr <= max_iterations or current_resid < error_limit or fit_residual < fit_error_limit ): # Update W: A=A.*(((W.*X)*Y')./((W.*(A*Y))*Y')); self . W *= np . dot ( masked_X , self . H . T ) / np . dot ( mask * np . dot ( self . W , self . H ), self . H . T ) self . W = np . maximum ( self . W , eps ) # Update H: Matlab: Y=Y.*((A'*(W.*X))./(A'*(W.*(A*Y)))); self . H *= np . dot ( self . W . T , masked_X ) / np . dot ( self . W . T , mask * np . dot ( self . W , self . H ) ) self . H = np . maximum ( self . H , eps ) # Evaluate X_est = np . dot ( self . W , self . H ) # This is basically error gradient for convergence purposes err = mask * ( X_est_prev - X_est ) fit_residual = np . sqrt ( np . sum ( err ** 2 )) # Save this iteration's predictions X_est_prev = X_est # Update the residuals; note that the norm = RMSE not MSE current_resid = np . linalg . norm ( masked_X - mask * X_est , ord = \"fro\" ) # Norm the residual with respect to the max of the dataset so we can use a common convergence threshold current_resid /= masked_X . max () if save_learning : self . error_history . append ( current_resid ) # curRes = linalg.norm(mask * (masked_X - X_est), ord='fro') if ctr % 10 == 0 and verbose : print ( \" \\t Current Iteration {} :\" . format ( ctr )) print ( \" \\t fit residual\" , np . round ( fit_residual , 4 )) ctr += 1 self . is_fit = True","title":"fit()"},{"location":"api/nmf_m/#emotioncf.models.NNMF_mult.predict","text":"Predict Subject's missing items using NNMF with multiplicative updating Returns: Type Description predicted_rating (Dataframe) adds field to object instance Source code in emotioncf/models.py def predict ( self ): \"\"\"Predict Subject's missing items using NNMF with multiplicative updating Returns: predicted_rating (Dataframe): adds field to object instance \"\"\" if not self . is_fit : raise ValueError ( \"You must fit() model first before using this method.\" ) self . predictions = self . data . copy () self . predictions . loc [:, :] = np . dot ( self . W , self . H ) self . is_predict = True","title":"predict()"},{"location":"api/nmf_s/","text":"emotioncf.models.NNMF_sgd The Non-negative Matrix Factorization algorithm tries to decompose a users x items matrix into two additional matrices: users x factors and factors x items. Training is performed via stochastic-gradient-descent. Unlike NNMF_mult errors during training are used to update latent factors separately for each user/item combination. The number of factors, convergence, and maximum iterations can be controlled with the n_factors , tol , and max_iterations arguments to the .fit method. By default the number of factors = the number items. fit ( self , n_factors = None , item_fact_reg = 0.0 , user_fact_reg = 0.0 , item_bias_reg = 0.0 , user_bias_reg = 0.0 , learning_rate = 0.001 , n_iterations = 5000 , tol = 0.001 , verbose = False , dilate_ts_n_samples = None , save_learning = True , fast_sgd = False , ** kwargs ) Fit NNMF collaborative filtering model using stochastic-gradient-descent Parameters: Name Type Description Default n_factors int number of factors to learn. Defaults to None which includes all factors. None item_fact_reg float item factor regularization to apply. Defaults to 0.0. 0.0 user_fact_reg float user factor regularization to apply. Defaults to 0.0. 0.0 item_bias_reg float item factor bias term to apply. Defaults to 0.0. 0.0 user_bias_reg float user factor bias term to apply. Defaults to 0.0. 0.0 learning_rate float how quickly to integrate errors during training. Defaults to 0.001. 0.001 n_iterations int total number of training iterations if convergence is not achieved. Defaults to 5000. 5000 tol float Convergence criteria. Model is considered converged if the change in error during training < tol. Defaults to 0.001. 0.001 verbose bool print information about training. Defaults to False. False dilate_ts_n_samples int How many items to dilate by prior to training. Defaults to None. None save_learning bool Save error for each training iteration for diagnostic purposes. Set this to False if memory is a limitation and the n_iterations is very large. Defaults to True. True fast_sdg bool; optional Use an JIT compiled SGD for faster fitting. Note that verbose outputs are not compatible with this option and error history is always saved; Default False required Source code in emotioncf/models.py def fit ( self , n_factors = None , item_fact_reg = 0.0 , user_fact_reg = 0.0 , item_bias_reg = 0.0 , user_bias_reg = 0.0 , learning_rate = 0.001 , n_iterations = 5000 , tol = 0.001 , verbose = False , dilate_ts_n_samples = None , save_learning = True , fast_sgd = False , ** kwargs , ): \"\"\" Fit NNMF collaborative filtering model using stochastic-gradient-descent Args: n_factors (int, optional): number of factors to learn. Defaults to None which includes all factors. item_fact_reg (float, optional): item factor regularization to apply. Defaults to 0.0. user_fact_reg (float, optional): user factor regularization to apply. Defaults to 0.0. item_bias_reg (float, optional): item factor bias term to apply. Defaults to 0.0. user_bias_reg (float, optional): user factor bias term to apply. Defaults to 0.0. learning_rate (float, optional): how quickly to integrate errors during training. Defaults to 0.001. n_iterations (int, optional): total number of training iterations if convergence is not achieved. Defaults to 5000. tol (float, optional): Convergence criteria. Model is considered converged if the change in error during training < tol. Defaults to 0.001. verbose (bool, optional): print information about training. Defaults to False. dilate_ts_n_samples (int, optional): How many items to dilate by prior to training. Defaults to None. save_learning (bool, optional): Save error for each training iteration for diagnostic purposes. Set this to False if memory is a limitation and the n_iterations is very large. Defaults to True. fast_sdg (bool; optional): Use an JIT compiled SGD for faster fitting. Note that verbose outputs are not compatible with this option and error history is always saved; Default False \"\"\" # initialize variables n_users , n_items = self . data . shape if n_factors is None : n_factors = n_items self . n_factors = n_factors if dilate_ts_n_samples is not None : self . _dilate_ts_rating_samples ( n_samples = dilate_ts_n_samples ) if self . is_mask : if self . is_mask_dilated : data = self . masked_data [ self . dilated_mask ] sample_row , sample_col = self . dilated_mask . values . nonzero () self . global_bias = data [ self . dilated_mask ] . mean () . mean () else : data = self . masked_data [ self . train_mask ] sample_row , sample_col = self . train_mask . values . nonzero () self . global_bias = data [ self . train_mask ] . mean () . mean () else : data = self . data . copy () sample_row , sample_col = zip ( * np . argwhere ( ~ np . isnan ( data . values ))) self . global_bias = data . values [ ~ np . isnan ( data . values )] . mean () # Convert tuples in case user asks for fast sgd cause numba complains sample_row , sample_col = np . array ( sample_row ), np . array ( sample_col ) # initialize latent vectors self . user_vecs = np . random . normal ( scale = 1.0 / n_factors , size = ( n_users , n_factors ) ) self . item_vecs = np . random . normal ( scale = 1.0 / n_factors , size = ( n_items , n_factors ) ) # Initialize biases self . user_bias = np . zeros ( n_users ) self . item_bias = np . zeros ( n_items ) self . item_fact_reg = item_fact_reg self . user_fact_reg = user_fact_reg self . item_bias_reg = item_bias_reg self . user_bias_reg = user_bias_reg # train weights ctr = 1 last_e = 0 delta = np . inf max_norm = data . abs () . max () . max () self . error_history = [] converged = False norm_e = np . inf if fast_sgd : ( error_history , converged , ctr , delta , norm_e , user_bias , user_vecs , item_bias , item_vecs , ) = sgd ( data . to_numpy (), self . global_bias , max_norm , tol , self . user_bias , self . user_vecs , self . user_bias_reg , self . user_fact_reg , self . item_bias , self . item_vecs , self . item_bias_reg , self . item_fact_reg , n_iterations , sample_row , sample_col , learning_rate , verbose , ) # Save outputs to model ( self . error_history , self . user_bias , self . user_vecs , self . item_bias , self . item_vecs , ) = ( error_history , user_bias , user_vecs , item_bias , item_vecs , ) else : with tqdm ( total = n_iterations ) as t : for ctr in range ( 1 , n_iterations + 1 ): if verbose : t . set_description ( f \"Norm Error: { np . round ( 100 * norm_e , 2 ) } % Delta Convg: { np . round ( delta , 4 ) } || { tol } \" ) # t.set_postfix(Delta=f\"{np.round(delta, 4)}\") training_indices = np . arange ( len ( sample_row )) np . random . shuffle ( training_indices ) for idx in training_indices : u = sample_row [ idx ] i = sample_col [ idx ] prediction = self . _predict_single ( u , i ) # Use changes in e to determine tolerance e = data . iloc [ u , i ] - prediction # error # Update biases self . user_bias [ u ] += learning_rate * ( e - self . user_bias_reg * self . user_bias [ u ] ) self . item_bias [ i ] += learning_rate * ( e - self . item_bias_reg * self . item_bias [ i ] ) # Update latent factors self . user_vecs [ u , :] += learning_rate * ( e * self . item_vecs [ i , :] - self . user_fact_reg * self . user_vecs [ u , :] ) self . item_vecs [ i , :] += learning_rate * ( e * self . user_vecs [ u , :] - self . item_fact_reg * self . item_vecs [ i , :] ) # Normalize the current error with respect to the max of the dataset norm_e = np . abs ( e ) / max_norm # Compute the delta delta = np . abs ( np . abs ( norm_e ) - np . abs ( last_e )) if save_learning : self . error_history . append ( norm_e ) if delta < tol : converged = True break t . update () # Save the last normalize error last_e = norm_e self . is_fit = True self . _n_iter = ctr self . _delta = delta self . _norm_e = norm_e self . converged = converged if verbose : if self . converged : print ( \" \\n\\t CONVERGED!\" ) print ( f \" \\n\\t Final Iteration: { self . _n_iter } \" ) print ( f \" \\t Final Delta: { np . round ( self . _delta ) } \" ) else : print ( \" \\t FAILED TO CONVERGE (n_iter reached)\" ) print ( f \" \\n\\t Final Iteration: { self . _n_iter } \" ) print ( f \" \\t Final delta exceeds tol: { tol } <= { np . round ( self . _delta , 5 ) } \" ) print ( f \" \\t Final Norm Error: { np . round ( 100 * norm_e , 2 ) } %\" ) predict ( self ) Predict Subject's missing items using NNMF with stochastic gradient descent Returns: Type Description predicted_rating (Dataframe) adds field to object instance Source code in emotioncf/models.py def predict ( self ): \"\"\"Predict Subject's missing items using NNMF with stochastic gradient descent Returns: predicted_rating (Dataframe): adds field to object instance \"\"\" self . predictions = self . data . copy () for u in range ( self . user_vecs . shape [ 0 ]): for i in range ( self . item_vecs . shape [ 0 ]): self . predictions . iloc [ u , i ] = self . _predict_single ( u , i ) self . is_predict = True","title":"NNMF_sgd"},{"location":"api/nmf_s/#emotioncfmodelsnnmf_sgd","text":"","title":"emotioncf.models.NNMF_sgd"},{"location":"api/nmf_s/#emotioncf.models.NNMF_sgd","text":"The Non-negative Matrix Factorization algorithm tries to decompose a users x items matrix into two additional matrices: users x factors and factors x items. Training is performed via stochastic-gradient-descent. Unlike NNMF_mult errors during training are used to update latent factors separately for each user/item combination. The number of factors, convergence, and maximum iterations can be controlled with the n_factors , tol , and max_iterations arguments to the .fit method. By default the number of factors = the number items.","title":"emotioncf.models.NNMF_sgd"},{"location":"api/nmf_s/#emotioncf.models.NNMF_sgd.fit","text":"Fit NNMF collaborative filtering model using stochastic-gradient-descent Parameters: Name Type Description Default n_factors int number of factors to learn. Defaults to None which includes all factors. None item_fact_reg float item factor regularization to apply. Defaults to 0.0. 0.0 user_fact_reg float user factor regularization to apply. Defaults to 0.0. 0.0 item_bias_reg float item factor bias term to apply. Defaults to 0.0. 0.0 user_bias_reg float user factor bias term to apply. Defaults to 0.0. 0.0 learning_rate float how quickly to integrate errors during training. Defaults to 0.001. 0.001 n_iterations int total number of training iterations if convergence is not achieved. Defaults to 5000. 5000 tol float Convergence criteria. Model is considered converged if the change in error during training < tol. Defaults to 0.001. 0.001 verbose bool print information about training. Defaults to False. False dilate_ts_n_samples int How many items to dilate by prior to training. Defaults to None. None save_learning bool Save error for each training iteration for diagnostic purposes. Set this to False if memory is a limitation and the n_iterations is very large. Defaults to True. True fast_sdg bool; optional Use an JIT compiled SGD for faster fitting. Note that verbose outputs are not compatible with this option and error history is always saved; Default False required Source code in emotioncf/models.py def fit ( self , n_factors = None , item_fact_reg = 0.0 , user_fact_reg = 0.0 , item_bias_reg = 0.0 , user_bias_reg = 0.0 , learning_rate = 0.001 , n_iterations = 5000 , tol = 0.001 , verbose = False , dilate_ts_n_samples = None , save_learning = True , fast_sgd = False , ** kwargs , ): \"\"\" Fit NNMF collaborative filtering model using stochastic-gradient-descent Args: n_factors (int, optional): number of factors to learn. Defaults to None which includes all factors. item_fact_reg (float, optional): item factor regularization to apply. Defaults to 0.0. user_fact_reg (float, optional): user factor regularization to apply. Defaults to 0.0. item_bias_reg (float, optional): item factor bias term to apply. Defaults to 0.0. user_bias_reg (float, optional): user factor bias term to apply. Defaults to 0.0. learning_rate (float, optional): how quickly to integrate errors during training. Defaults to 0.001. n_iterations (int, optional): total number of training iterations if convergence is not achieved. Defaults to 5000. tol (float, optional): Convergence criteria. Model is considered converged if the change in error during training < tol. Defaults to 0.001. verbose (bool, optional): print information about training. Defaults to False. dilate_ts_n_samples (int, optional): How many items to dilate by prior to training. Defaults to None. save_learning (bool, optional): Save error for each training iteration for diagnostic purposes. Set this to False if memory is a limitation and the n_iterations is very large. Defaults to True. fast_sdg (bool; optional): Use an JIT compiled SGD for faster fitting. Note that verbose outputs are not compatible with this option and error history is always saved; Default False \"\"\" # initialize variables n_users , n_items = self . data . shape if n_factors is None : n_factors = n_items self . n_factors = n_factors if dilate_ts_n_samples is not None : self . _dilate_ts_rating_samples ( n_samples = dilate_ts_n_samples ) if self . is_mask : if self . is_mask_dilated : data = self . masked_data [ self . dilated_mask ] sample_row , sample_col = self . dilated_mask . values . nonzero () self . global_bias = data [ self . dilated_mask ] . mean () . mean () else : data = self . masked_data [ self . train_mask ] sample_row , sample_col = self . train_mask . values . nonzero () self . global_bias = data [ self . train_mask ] . mean () . mean () else : data = self . data . copy () sample_row , sample_col = zip ( * np . argwhere ( ~ np . isnan ( data . values ))) self . global_bias = data . values [ ~ np . isnan ( data . values )] . mean () # Convert tuples in case user asks for fast sgd cause numba complains sample_row , sample_col = np . array ( sample_row ), np . array ( sample_col ) # initialize latent vectors self . user_vecs = np . random . normal ( scale = 1.0 / n_factors , size = ( n_users , n_factors ) ) self . item_vecs = np . random . normal ( scale = 1.0 / n_factors , size = ( n_items , n_factors ) ) # Initialize biases self . user_bias = np . zeros ( n_users ) self . item_bias = np . zeros ( n_items ) self . item_fact_reg = item_fact_reg self . user_fact_reg = user_fact_reg self . item_bias_reg = item_bias_reg self . user_bias_reg = user_bias_reg # train weights ctr = 1 last_e = 0 delta = np . inf max_norm = data . abs () . max () . max () self . error_history = [] converged = False norm_e = np . inf if fast_sgd : ( error_history , converged , ctr , delta , norm_e , user_bias , user_vecs , item_bias , item_vecs , ) = sgd ( data . to_numpy (), self . global_bias , max_norm , tol , self . user_bias , self . user_vecs , self . user_bias_reg , self . user_fact_reg , self . item_bias , self . item_vecs , self . item_bias_reg , self . item_fact_reg , n_iterations , sample_row , sample_col , learning_rate , verbose , ) # Save outputs to model ( self . error_history , self . user_bias , self . user_vecs , self . item_bias , self . item_vecs , ) = ( error_history , user_bias , user_vecs , item_bias , item_vecs , ) else : with tqdm ( total = n_iterations ) as t : for ctr in range ( 1 , n_iterations + 1 ): if verbose : t . set_description ( f \"Norm Error: { np . round ( 100 * norm_e , 2 ) } % Delta Convg: { np . round ( delta , 4 ) } || { tol } \" ) # t.set_postfix(Delta=f\"{np.round(delta, 4)}\") training_indices = np . arange ( len ( sample_row )) np . random . shuffle ( training_indices ) for idx in training_indices : u = sample_row [ idx ] i = sample_col [ idx ] prediction = self . _predict_single ( u , i ) # Use changes in e to determine tolerance e = data . iloc [ u , i ] - prediction # error # Update biases self . user_bias [ u ] += learning_rate * ( e - self . user_bias_reg * self . user_bias [ u ] ) self . item_bias [ i ] += learning_rate * ( e - self . item_bias_reg * self . item_bias [ i ] ) # Update latent factors self . user_vecs [ u , :] += learning_rate * ( e * self . item_vecs [ i , :] - self . user_fact_reg * self . user_vecs [ u , :] ) self . item_vecs [ i , :] += learning_rate * ( e * self . user_vecs [ u , :] - self . item_fact_reg * self . item_vecs [ i , :] ) # Normalize the current error with respect to the max of the dataset norm_e = np . abs ( e ) / max_norm # Compute the delta delta = np . abs ( np . abs ( norm_e ) - np . abs ( last_e )) if save_learning : self . error_history . append ( norm_e ) if delta < tol : converged = True break t . update () # Save the last normalize error last_e = norm_e self . is_fit = True self . _n_iter = ctr self . _delta = delta self . _norm_e = norm_e self . converged = converged if verbose : if self . converged : print ( \" \\n\\t CONVERGED!\" ) print ( f \" \\n\\t Final Iteration: { self . _n_iter } \" ) print ( f \" \\t Final Delta: { np . round ( self . _delta ) } \" ) else : print ( \" \\t FAILED TO CONVERGE (n_iter reached)\" ) print ( f \" \\n\\t Final Iteration: { self . _n_iter } \" ) print ( f \" \\t Final delta exceeds tol: { tol } <= { np . round ( self . _delta , 5 ) } \" ) print ( f \" \\t Final Norm Error: { np . round ( 100 * norm_e , 2 ) } %\" )","title":"fit()"},{"location":"api/nmf_s/#emotioncf.models.NNMF_sgd.predict","text":"Predict Subject's missing items using NNMF with stochastic gradient descent Returns: Type Description predicted_rating (Dataframe) adds field to object instance Source code in emotioncf/models.py def predict ( self ): \"\"\"Predict Subject's missing items using NNMF with stochastic gradient descent Returns: predicted_rating (Dataframe): adds field to object instance \"\"\" self . predictions = self . data . copy () for u in range ( self . user_vecs . shape [ 0 ]): for i in range ( self . item_vecs . shape [ 0 ]): self . predictions . iloc [ u , i ] = self . _predict_single ( u , i ) self . is_predict = True","title":"predict()"},{"location":"api/utils/","text":"emotioncf.utils Utility functions and helpers create_sub_by_item_matrix ( df , columns = None , force_float = True , errors = 'raise' ) Convert a pandas long data frame of a single rating into a subject by item matrix Parameters: Name Type Description Default df Dataframe input dataframe required columns list list of length 3 with dataframe columns to use for reshaping. The first value should reflect unique individual identifier (\"Subject\"), the second a unique item identifier (\"Item\", \"Timepoint\"), and the last the rating made by the individual on that item (\"Rating\"). Defaults to [\"Subject\", \"Item\", \"Rating\"] None force_float bool force the resulting output to be float data types with errors being set to NaN; Default True True errors string how to handle errors in pd.to_numeric; Default 'raise' 'raise' Returns: Type Description pd.DataFrame user x item rating Dataframe Source code in emotioncf/utils.py def create_sub_by_item_matrix ( df , columns = None , force_float = True , errors = \"raise\" ): \"\"\"Convert a pandas long data frame of a single rating into a subject by item matrix Args: df (Dataframe): input dataframe columns (list): list of length 3 with dataframe columns to use for reshaping. The first value should reflect unique individual identifier (\"Subject\"), the second a unique item identifier (\"Item\", \"Timepoint\"), and the last the rating made by the individual on that item (\"Rating\"). Defaults to [\"Subject\", \"Item\", \"Rating\"] force_float (bool): force the resulting output to be float data types with errors being set to NaN; Default True errors (string): how to handle errors in pd.to_numeric; Default 'raise' Return: pd.DataFrame: user x item rating Dataframe \"\"\" if not isinstance ( df , pd . DataFrame ): raise ValueError ( \"df must be pandas instance\" ) if columns is None : columns = [ \"Subject\" , \"Item\" , \"Rating\" ] if not all ([ x in df . columns for x in columns ]): raise ValueError ( f \"df is missing some or all of the following columns: { columns } \" ) ratings = df [ columns ] ratings = ratings . pivot ( index = columns [ 0 ], columns = columns [ 1 ], values = columns [ 2 ]) try : if force_float : ratings = ratings . apply ( pd . to_numeric , errors = errors ) except ValueError as e : print ( \"Auto-converting data to floats failed, probably because you have non-numeric data in some rows. You can set errors = 'coerce' to set these failures to NaN\" ) raise ( e ) return ratings get_size_in_mb ( arr ) Calculates size of ndarray in megabytes Source code in emotioncf/utils.py def get_size_in_mb ( arr ): \"\"\"Calculates size of ndarray in megabytes\"\"\" if isinstance ( arr , ( np . ndarray , csr_matrix )): return arr . data . nbytes / 1e6 else : raise TypeError ( \"input must by a numpy array or scipy csr sparse matrix\" ) get_sparsity ( arr ) Calculates sparsity of ndarray (0 - 1) Source code in emotioncf/utils.py def get_sparsity ( arr ): \"\"\"Calculates sparsity of ndarray (0 - 1)\"\"\" if isinstance ( arr , ( np . ndarray )): return 1 - ( np . count_nonzero ( arr ) / arr . size ) else : raise TypeError ( \"input must be a numpy array\" ) nanpdist ( arr , metric = 'euclidean' , return_square = True ) Just like scipy.spatial.distance.pdist or sklearn.metrics.pairwise_distances, but respects NaNs by only comparing the overlapping values from pairs of rows. Parameters: Name Type Description Default arr np.ndarray 2d array required metric str; optional distance metric to use. Must be supported by scipy 'euclidean' return_square boo; optional return a symmetric 2d distance matrix like sklearn instead of a 1d vector like pdist; Default True True Returns: Type Description np.ndarray symmetric 2d array of distances Source code in emotioncf/utils.py def nanpdist ( arr , metric = \"euclidean\" , return_square = True ): \"\"\" Just like scipy.spatial.distance.pdist or sklearn.metrics.pairwise_distances, but respects NaNs by only comparing the overlapping values from pairs of rows. Args: arr (np.ndarray): 2d array metric (str; optional): distance metric to use. Must be supported by scipy return_square (boo; optional): return a symmetric 2d distance matrix like sklearn instead of a 1d vector like pdist; Default True Return: np.ndarray: symmetric 2d array of distances \"\"\" has_nans = pd . DataFrame ( arr ) . isnull () . any () . any () if not has_nans : out = pdist ( arr , metric = metric ) else : nrows = arr . shape [ 0 ] out = np . zeros ( nrows * ( nrows - 1 ) // 2 , dtype = float ) mask = np . isfinite ( arr ) vec_mask = np . zeros ( arr . shape [ 1 ], dtype = bool ) k = 0 for row1_idx in range ( nrows - 1 ): for row2_idx in range ( row1_idx + 1 , nrows ): vec_mask = np . logical_and ( mask [ row1_idx ], mask [ row2_idx ]) masked_row1 , masked_row2 = ( arr [ row1_idx ][ vec_mask ], arr [ row2_idx ][ vec_mask ], ) out [ k ] = pdist ( np . vstack ([ masked_row1 , masked_row2 ]), metric = metric ) k += 1 if return_square : if out . ndim == 1 : out = squareform ( out ) else : if out . ndim == 2 : out = squareform ( out ) return out","title":"emotioncf.utils"},{"location":"api/utils/#emotioncfutils","text":"","title":"emotioncf.utils"},{"location":"api/utils/#emotioncf.utils","text":"Utility functions and helpers","title":"emotioncf.utils"},{"location":"api/utils/#emotioncf.utils.create_sub_by_item_matrix","text":"Convert a pandas long data frame of a single rating into a subject by item matrix Parameters: Name Type Description Default df Dataframe input dataframe required columns list list of length 3 with dataframe columns to use for reshaping. The first value should reflect unique individual identifier (\"Subject\"), the second a unique item identifier (\"Item\", \"Timepoint\"), and the last the rating made by the individual on that item (\"Rating\"). Defaults to [\"Subject\", \"Item\", \"Rating\"] None force_float bool force the resulting output to be float data types with errors being set to NaN; Default True True errors string how to handle errors in pd.to_numeric; Default 'raise' 'raise' Returns: Type Description pd.DataFrame user x item rating Dataframe Source code in emotioncf/utils.py def create_sub_by_item_matrix ( df , columns = None , force_float = True , errors = \"raise\" ): \"\"\"Convert a pandas long data frame of a single rating into a subject by item matrix Args: df (Dataframe): input dataframe columns (list): list of length 3 with dataframe columns to use for reshaping. The first value should reflect unique individual identifier (\"Subject\"), the second a unique item identifier (\"Item\", \"Timepoint\"), and the last the rating made by the individual on that item (\"Rating\"). Defaults to [\"Subject\", \"Item\", \"Rating\"] force_float (bool): force the resulting output to be float data types with errors being set to NaN; Default True errors (string): how to handle errors in pd.to_numeric; Default 'raise' Return: pd.DataFrame: user x item rating Dataframe \"\"\" if not isinstance ( df , pd . DataFrame ): raise ValueError ( \"df must be pandas instance\" ) if columns is None : columns = [ \"Subject\" , \"Item\" , \"Rating\" ] if not all ([ x in df . columns for x in columns ]): raise ValueError ( f \"df is missing some or all of the following columns: { columns } \" ) ratings = df [ columns ] ratings = ratings . pivot ( index = columns [ 0 ], columns = columns [ 1 ], values = columns [ 2 ]) try : if force_float : ratings = ratings . apply ( pd . to_numeric , errors = errors ) except ValueError as e : print ( \"Auto-converting data to floats failed, probably because you have non-numeric data in some rows. You can set errors = 'coerce' to set these failures to NaN\" ) raise ( e ) return ratings","title":"create_sub_by_item_matrix()"},{"location":"api/utils/#emotioncf.utils.get_size_in_mb","text":"Calculates size of ndarray in megabytes Source code in emotioncf/utils.py def get_size_in_mb ( arr ): \"\"\"Calculates size of ndarray in megabytes\"\"\" if isinstance ( arr , ( np . ndarray , csr_matrix )): return arr . data . nbytes / 1e6 else : raise TypeError ( \"input must by a numpy array or scipy csr sparse matrix\" )","title":"get_size_in_mb()"},{"location":"api/utils/#emotioncf.utils.get_sparsity","text":"Calculates sparsity of ndarray (0 - 1) Source code in emotioncf/utils.py def get_sparsity ( arr ): \"\"\"Calculates sparsity of ndarray (0 - 1)\"\"\" if isinstance ( arr , ( np . ndarray )): return 1 - ( np . count_nonzero ( arr ) / arr . size ) else : raise TypeError ( \"input must be a numpy array\" )","title":"get_sparsity()"},{"location":"api/utils/#emotioncf.utils.nanpdist","text":"Just like scipy.spatial.distance.pdist or sklearn.metrics.pairwise_distances, but respects NaNs by only comparing the overlapping values from pairs of rows. Parameters: Name Type Description Default arr np.ndarray 2d array required metric str; optional distance metric to use. Must be supported by scipy 'euclidean' return_square boo; optional return a symmetric 2d distance matrix like sklearn instead of a 1d vector like pdist; Default True True Returns: Type Description np.ndarray symmetric 2d array of distances Source code in emotioncf/utils.py def nanpdist ( arr , metric = \"euclidean\" , return_square = True ): \"\"\" Just like scipy.spatial.distance.pdist or sklearn.metrics.pairwise_distances, but respects NaNs by only comparing the overlapping values from pairs of rows. Args: arr (np.ndarray): 2d array metric (str; optional): distance metric to use. Must be supported by scipy return_square (boo; optional): return a symmetric 2d distance matrix like sklearn instead of a 1d vector like pdist; Default True Return: np.ndarray: symmetric 2d array of distances \"\"\" has_nans = pd . DataFrame ( arr ) . isnull () . any () . any () if not has_nans : out = pdist ( arr , metric = metric ) else : nrows = arr . shape [ 0 ] out = np . zeros ( nrows * ( nrows - 1 ) // 2 , dtype = float ) mask = np . isfinite ( arr ) vec_mask = np . zeros ( arr . shape [ 1 ], dtype = bool ) k = 0 for row1_idx in range ( nrows - 1 ): for row2_idx in range ( row1_idx + 1 , nrows ): vec_mask = np . logical_and ( mask [ row1_idx ], mask [ row2_idx ]) masked_row1 , masked_row2 = ( arr [ row1_idx ][ vec_mask ], arr [ row2_idx ][ vec_mask ], ) out [ k ] = pdist ( np . vstack ([ masked_row1 , masked_row2 ]), metric = metric ) k += 1 if return_square : if out . ndim == 1 : out = squareform ( out ) else : if out . ndim == 2 : out = squareform ( out ) return out","title":"nanpdist()"}]}